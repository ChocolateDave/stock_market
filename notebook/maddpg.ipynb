{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-Agent Deep Deterministic Policy Gradient for Stock Market"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stock Market Environment\n",
    "\n",
    "- __Hyperparameters__\n",
    "- __Observation Space__\n",
    "  - `stock_price`: `ndarray` of shape $[N_{stock}, ]$\n",
    "  - `correlated_stock`: `ndarray` of shape $[N_{correlated}, ]$\n",
    "  - `uncorrelated_stock`: `ndarray` of shape $[N_{uncorrelated}, ]$\n",
    "  - `budgets`: `ndarray` of shape $[N_{agents}, ]$\n",
    "  - `shares_held`: `ndarray` of shape $[N_{agents}, ]$\n",
    "  - `agent_views`: `ndarray` of shape $[N_{agents}, N_{stock}]$\n",
    "  - `company_states`: `ndarray` of shape $[N_{company}, ]$\n",
    "- __Action Space__\n",
    "  - dimension_1: log buy/sell prices $\\log p\\in\\left(-\\infty, +\\infty\\right)$ => `gym.spaces.Box`\n",
    "  - dimension_2: discrete shares $s\\in\\mathbb{N}$ => `gym.spaces.Discrete`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, Optional, Sequence, Tuple, Union\n",
    "\n",
    "import numpy as np\n",
    "from gym.core import ActType, ObsType, Env\n",
    "from gym.spaces import Box, MultiDiscrete, Tuple as TupleSpace\n",
    "\n",
    "\n",
    "class StockMarketEnv(Env):\n",
    "    \n",
    "    def __init__(self,\n",
    "                 num_agents: int,\n",
    "                 budge_discount: float = 0.9,\n",
    "                 num_company: int = 5,\n",
    "                 num_correlated_stocks: int = 19,\n",
    "                 num_uncorrelated_stocks: int = 10,\n",
    "                 max_shares: int = 100000,\n",
    "                 start_prices: Union[float, Sequence[float]] = 100.0,\n",
    "                 min_budget: float = 100.0,\n",
    "                 max_budget: float = 10000.0,\n",
    "                 budget_discount: float = 0.9,\n",
    "                 step_size: float = 1.0,\n",
    "                 price_std: float = 100.0,\n",
    "                 noise_std: float = 10.0,\n",
    "                 seed: int = 0) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        # Agent Parameters\n",
    "        self.num_agents = num_agents\n",
    "        self.num_company = num_company\n",
    "        self.min_budget = min_budget\n",
    "        self.max_budget = max_budget\n",
    "        self.budget_discount = budge_discount\n",
    "        self.max_shares = max_shares\n",
    "\n",
    "        # Stock Market Parameters\n",
    "        self.dt = step_size\n",
    "        self.start_prices = start_prices\n",
    "        self.price_std = price_std\n",
    "        self.noise_std = noise_std\n",
    "\n",
    "        # Observation and Action spaces\n",
    "        self.n_correlated_stocks = num_correlated_stocks\n",
    "        self.n_uncorrelated_stocks = num_uncorrelated_stocks\n",
    "        self.n_stocks = num_correlated_stocks + num_uncorrelated_stocks + 1\n",
    "        self.observation_space = Box(low=0.0,\n",
    "                                     high=float(\"inf\"),\n",
    "                                     shape=(self.num_agents, self.n_stocks))\n",
    "        self.action_space = TupleSpace(\n",
    "            (Box(low=-float(\"inf\"),\n",
    "                 high=float(\"inf\"),\n",
    "                 shape=(self.num_agents, self.n_stocks)),\n",
    "             MultiDiscrete([[max_shares] * self.n_stocks] * self.num_agents))\n",
    "        )\n",
    "        self._seed = seed\n",
    "\n",
    "    def reset(self,\n",
    "              seed: Optional[int] = None,\n",
    "              return_info: bool = True) -> Tuple[ObsType, Dict]:\n",
    "        self.rng = np.random.default_rng(seed=seed or self._seed)        \n",
    "\n",
    "        correlated_stocks = np.clip(\n",
    "            np.random.normal(loc=self.start_prices,\n",
    "                             scale=self.price_std,\n",
    "                             size=(self.n_correlated_stocks, )),\n",
    "            a_min=1, a_max=None\n",
    "        )\n",
    "        uncorrelated_stocks = np.clip(\n",
    "            np.random.normal(loc=self.start_prices,\n",
    "                             scale=self.price_std,\n",
    "                             size=(self.n_uncorrelated_stocks,)),\n",
    "            a_min=1, a_max=None\n",
    "        )\n",
    "        self.eta = np.clip(\n",
    "            np.random.normal(loc=1.5, scale=1.5, size=(self.num_agents, )),\n",
    "            a_min=0, a_max=10\n",
    "        )\n",
    "        self.valid_mask = np.zeros(shape=(self.num_agents, self.n_stocks),\n",
    "                                   dtype=\"bool\")\n",
    "        self.valid_mask[:, 1:1+self.n_correlated_stocks] = True\n",
    "        self.valid_mask[self.rng.integers(low=0, high=self.num_agents),\n",
    "                        1 + self.n_correlated_stocks:] = True\n",
    "\n",
    "        self.prices = np.asarray(self.start_prices)\n",
    "        self.budgets = self.min_budget + self.rng.random(\n",
    "            size=(self.num_agents), dtype=\"float32\") * (\n",
    "                self.max_budget - self.min_budget)\n",
    "        self.shares = self.rng.integers(low=1,\n",
    "                                        high=self.max_shares,\n",
    "                                        size=(self.num_agents, self.n_stocks))\n",
    "\n",
    "        return (self.prices,\n",
    "                {\n",
    "                    \"correlated_stocks\": correlated_stocks,\n",
    "                    \"uncorrelated_stocks\": uncorrelated_stocks,\n",
    "                    \"budgets\": self.budgets,\n",
    "                    \"shares\": self.shares,\n",
    "                    \"valid_mask\": self.valid_mask,\n",
    "                    \"company_states\": None  # TODO: Company states\n",
    "                })\n",
    "    \n",
    "    def is_terminated(self) -> bool:\n",
    "        return True\n",
    "\n",
    "    def step(self, action: Tuple[np.ndarray, np.ndarray]) -> Tuple:\n",
    "        assert (len(action) == 2 and\n",
    "                action[0].shape == (self.num_agents, self.n_stocks) and\n",
    "                action[1].shape == (self.num_agents, self.n_stocks))\n",
    "        # TODO\n",
    "        proposed_prices = 1. + np.exp(action[0])\n",
    "        proposed_shares = action[1]\n",
    "\n",
    "        # Update budgets and shares\n",
    "        potential_budgets = self.budgets + \\\n",
    "            (proposed_prices * (-proposed_shares)).sum(-1)\n",
    "        potential_shares = self.shares + proposed_shares\n",
    "        print(\"Current budgets: \\n\", potential_budgets,\n",
    "              \"\\nCurrent shares: \\n\", potential_shares)\n",
    "        rewards = np.where(\n",
    "            np.logical_or(potential_budgets < 0.0,\n",
    "                          np.any(potential_shares < 0.0, axis=-1)),\n",
    "            -100, 0.0\n",
    "        )\n",
    "        print(\"Rewards\", rewards)\n",
    "        curr_prices = self.prices\n",
    "        \n",
    "        # TODO\n",
    "\n",
    "\n",
    "        # TODO\n",
    "\n",
    "        dones = self.is_terminated()\n",
    "        if dones:\n",
    "            next_s, _ = self.reset()\n",
    "\n",
    "        return \n",
    "\n",
    "    @staticmethod\n",
    "    def utility(c: float, eta: float) -> float:\n",
    "        if eta!= 1.0:\n",
    "            return (c ** (1.0 - eta) - 1.0) / (1.0 - eta)\n",
    "        else:\n",
    "            return np.log(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current budgets: \n",
      " [-27991.42362094 -34178.41267729 -35891.77252471 -37679.80823529\n",
      " -28668.28449202 -30585.09359443 -38112.77494562 -39650.0941211\n",
      " -34963.18385458 -40977.61369228] \n",
      "Current shares: \n",
      " [[1484 1084  874 1722  790  738  632  981 1233  964 1773 1594  170  739\n",
      "  1737 1070  954  836 1193 1766  763  630 1780  100  614  867  905  656\n",
      "   694  837]\n",
      " [ 901   65  285  760  834  718 1305 1028  896 1513 1300  732 1520 1772\n",
      "  1731  940  780 1930 1643 1055  899 1620  485 1600 1024 1432 1497  851\n",
      "   842 1002]\n",
      " [ 680  979 1288 1587 1220  662 1828  627 1331 1392  657  706 1110 1577\n",
      "   886 1109  394  844 1159  640 1636  907  771 1453 1416   84  534  670\n",
      "  1154 1186]\n",
      " [1769  664 1006  804 1715  909  749 1240  650 1330  673 1422 1099 1214\n",
      "  1009 1681  948  973  696  943  891 1376  603 1633  437 1555 1536 1478\n",
      "   885 1355]\n",
      " [1624 1135  920 1581 1328  710  748 1583  378  787  974  853  995 1090\n",
      "   681  976 1628  836  930  425 1392 1659 1175  384 1946  692 1394   18\n",
      "   644 1181]\n",
      " [1590 1305  886 1170 1428  606 1206  288  816 1016  976 1477  924 1459\n",
      "  1270  369 1060  913 1339  970 1669 1053 1552 1084  886  711  939  272\n",
      "  1023 1632]\n",
      " [1788  512  786 1009 1707  954 1411  886 1050  237  735  358  645 1041\n",
      "  1046  763 1554 1522  580 1238  762 1037  538  482 1283 1683  329  827\n",
      "   832  825]\n",
      " [ 713 1218  321 1319 1513 1039  106  428 1119  965  834 1046 1517  615\n",
      "   785 1418 1382 1453  731 1515 1308 1132  466 1492 1745 1052 1413 1402\n",
      "   908  397]\n",
      " [ 306 1140 1621  773  571  757  376  961  972 1670 1388 1837 1123 1391\n",
      "  1140  493  773 1320  791 1423  558  676 1310 1633 1136 1164 1564 1167\n",
      "  1164  282]\n",
      " [1470  827  597  125 1685 1603  263 1625  415  301 1712 1830  636  871\n",
      "  1303 1448 1249 1125 1200  522  826 1056 1144  988  798  972 1607  945\n",
      "   630 1488]]\n",
      "Rewards [-100. -100. -100. -100. -100. -100. -100. -100. -100. -100.]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(None, None)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = StockMarketEnv(10)\n",
    "env.reset()\n",
    "random_action = env.action_space.sample()\n",
    "\n",
    "env.step(random_action)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MADDPG Trainer\n",
    "\n",
    "The `MADDPG Trainer` class is a generic version of the `DDPG` trainer initialized with\n",
    "- A sequence of `DDPG Agent` class objects\n",
    "- A shared observation buffer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "from copy import deepcopy\n",
    "from typing import Any, Optional, Sequence, Tuple, Union\n",
    "\n",
    "import numpy as np\n",
    "import torch as th\n",
    "from numpy import ndarray\n",
    "from pettingzoo.mpe import simple_adversary_v2\n",
    "from torch import Tensor, nn, optim\n",
    "from torch.nn import functional as F\n",
    "\n",
    "device = th.device('cuda:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model\n",
    "class PolicyNet(nn.Module):\n",
    "\n",
    "    def __init__(self,\n",
    "                 in_features: int,\n",
    "                 action_size: int,\n",
    "                 num_hidden_1: int = 400,\n",
    "                 num_hidden_2: int = 300,\n",
    "                 negative_slope: float = 0.01) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.linear_1 = nn.Linear(in_features, num_hidden_1)\n",
    "        self.linear_2 = nn.Linear(num_hidden_1, num_hidden_2)\n",
    "        self.linear_3 = nn.Linear(num_hidden_2, action_size)\n",
    "        self.neg_slope = negative_slope\n",
    "\n",
    "        self.reset_parameters()\n",
    "    \n",
    "    def forward(self, obs: Tensor) -> Tensor:\n",
    "        obs = obs.float()\n",
    "        obs = F.leaky_relu(self.linear_1(obs), self.neg_slope)\n",
    "        obs = F.leaky_relu(self.linear_2(obs), self.neg_slope)\n",
    "        acs = th.tanh(self.linear_3(obs))\n",
    "        \n",
    "        return acs\n",
    "    \n",
    "    def reset_parameters(self) -> None:\n",
    "        gain_lrelu = nn.init.calculate_gain('leaky_relu')\n",
    "        gain_tanh = nn.init.calculate_gain('tanh')\n",
    "        nn.init.xavier_uniform_(self.linear_1.weight, gain=gain_lrelu)\n",
    "        nn.init.xavier_uniform_(self.linear_2.weight, gain=gain_lrelu)\n",
    "        nn.init.xavier_uniform_(self.linear_3.weight, gain=gain_tanh)\n",
    "\n",
    "\n",
    "class CriticNet(nn.Module):\n",
    "    \n",
    "    def __init__(self,\n",
    "                 obs_in_features: int,\n",
    "                 acs_in_features: int,\n",
    "                 num_hidden_1: int = 400,\n",
    "                 num_hidden_2: int = 300,\n",
    "                 negative_slope: float = 0.01) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.linear_1 = nn.Linear(obs_in_features, num_hidden_1)\n",
    "        self.linear_2 = nn.Linear(num_hidden_1 + acs_in_features, num_hidden_2)\n",
    "        self.linear_3 = nn.Linear(num_hidden_2, 1)\n",
    "        self.neg_slope = negative_slope\n",
    "\n",
    "    def forward(self, obs: Tensor, acs: Tensor) -> Tensor:\n",
    "        obs = obs.float()\n",
    "        acs = acs.float()\n",
    "\n",
    "        obs = F.leaky_relu(self.linear_1(obs), self.neg_slope)\n",
    "        q_val = F.leaky_relu(self.linear_2(th.cat([obs, acs], -1)),\n",
    "                             self.neg_slope)\n",
    "        q_val = self.linear_3(q_val)\n",
    "\n",
    "        return q_val\n",
    "    \n",
    "    def reset_parameters(self) -> None:\n",
    "        gain = nn.init.calculate_gain('leaky_relu')\n",
    "        nn.init.xavier_uniform_(self.linear_1.weight, gain)\n",
    "        nn.init.xavier_uniform_(self.linear_2.weight, gain)\n",
    "        nn.init.xavier_uniform_(self.linear_3.weight, gain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replay Buffer\n",
    "class ReplayBuffer:\n",
    "\n",
    "    def __init__(self, max_size: int = 1000000) -> None:\n",
    "        self._storage = []\n",
    "        self._max_size = max_size\n",
    "        self._ptr = 0\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self._storage)\n",
    "    \n",
    "    def clear(self) -> None:\n",
    "        self._storage.clear()\n",
    "        self._ptr = 0\n",
    "\n",
    "    def add(self,\n",
    "            obs: ndarray,\n",
    "            acs: ndarray,\n",
    "            next_obs: ndarray,\n",
    "            rewards: ndarray,\n",
    "            dones: ndarray) -> None:\n",
    "        _tuple = (obs, acs, next_obs, rewards, dones)\n",
    "\n",
    "        if self._ptr >= len(self._storage):\n",
    "            self._storage.append(_tuple)\n",
    "        else:\n",
    "            self._storage[self._ptr] = _tuple\n",
    "        \n",
    "        self._ptr = (self._ptr + 1) % self._max_size\n",
    "    \n",
    "    def sample(self,\n",
    "               agent_id: int,\n",
    "               batch_size: int = 64) -> Tuple[ndarray, ...]:\n",
    "\n",
    "        if batch_size > 0:\n",
    "            idcs = self._make_idcs(batch_size)\n",
    "        else:\n",
    "            idcs = range(0, len(self._storage))\n",
    "        \n",
    "        return self._encode_sample(idcs, agent_id)\n",
    "\n",
    "    def sample_index(self, idx: int) -> Tuple[ndarray, ...]:\n",
    "        return self._encode_sample(idx)\n",
    "\n",
    "    def _encode_sample(self,\n",
    "                       idcs: Union[int, Sequence[int]],\n",
    "                       agent_id: int) -> Tuple[ndarray, ...]:\n",
    "        obs, acs, next_obs, rewards, dones = [], [], [], [], []\n",
    "        for i in idcs:\n",
    "            ob, ac, next_ob, reward, done = self._storage[i]\n",
    "            obs.append(np.concatenate(ob[:]))\n",
    "            acs.append(ac)\n",
    "            next_obs.append(np.concatenate(next_ob[:]))\n",
    "            rewards.append(reward[agent_id])\n",
    "            dones.append(done[agent_id])\n",
    "        \n",
    "        return (np.asarray(obs, dtype='float32'),\n",
    "                np.asarray(acs, dtype='float32'),\n",
    "                np.asarray(next_obs, dtype='float32'),\n",
    "                np.asarray(rewards, dtype='float32'),\n",
    "                np.asarray(dones, dtype='float32'))\n",
    "\n",
    "    def _make_random_index(self, batch_size: int) -> Sequence[int]:\n",
    "        return [np.random.randint(0, len(self._storage) - 1)\n",
    "                for _ in range(batch_size)]\n",
    "\n",
    "    def _make_latest_index(self, batch_size: int) -> Sequence[int]:\n",
    "        idcs = [(self._ptr - 1 - i) % self._max_size\n",
    "                for i in range(batch_size)]\n",
    "        np.random.shuffle(idcs)\n",
    "\n",
    "        return idcs\n",
    "\n",
    "replay_buffer = ReplayBuffer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = simple_adversary_v2.env(N=2, max_cycles=25, continuous_actions=False)\n",
    "env.reset()\n",
    "# Initialize agents\n",
    "def hard_update(src: nn.Module,\n",
    "                tar: nn.Module,\n",
    "                non_blocking: bool = True) -> None:\n",
    "    with th.no_grad():\n",
    "        for param, tar_param in zip(src.parameters(), tar.parameters()):\n",
    "            param.data.copy_(tar_param, non_blocking)\n",
    "\n",
    "def soft_update(src: nn.Module,\n",
    "                tar: nn.Module,\n",
    "                tau: float = 0.001,\n",
    "                non_blocking: bool = True) -> None:\n",
    "    with th.no_grad():\n",
    "        for param, tar_param in zip(src.parameters(), tar.parameters()):\n",
    "            param.data.copy_(param * tau + tar_param * (1 - tau))\n",
    "\n",
    "policy_nets, critic_nets = [], []\n",
    "policy_tar_nets, critic_tar_nets = [], []\n",
    "policy_opts, critic_opts = [], []\n",
    "global_obs_size, global_acs_size = 0, 0\n",
    "for agent in env.agents:\n",
    "    if len(env.observation_space(agent).shape) > 2:\n",
    "        raise RuntimeError('Image inputs not supported')\n",
    "    global_obs_size += env.observation_space(agent).shape[0]\n",
    "    global_acs_size += env.action_space(agent).n\n",
    "\n",
    "for i, agent in enumerate(env.agents):\n",
    "    policy_nets.append(PolicyNet(env.observation_space(agent).shape[0],\n",
    "                                 env.action_space(agent).n).to(device))\n",
    "    policy_tar_nets.append(PolicyNet(env.observation_space(agent).shape[0],\n",
    "                                     env.action_space(agent).n).to(device))\n",
    "    hard_update(policy_tar_nets[i], policy_nets[i])\n",
    "    critic_nets.append(CriticNet(global_obs_size, global_acs_size).to(device))\n",
    "    critic_tar_nets.append(\n",
    "        CriticNet(global_obs_size, global_acs_size).to(device)\n",
    "    )\n",
    "    hard_update(critic_tar_nets[i], critic_nets[i])\n",
    "    policy_opts.append(optim.Adam(policy_nets[i].parameters(), lr=1e-4))\n",
    "    critic_opts.append(\n",
    "        optim.Adam(critic_nets[i].parameters(), lr=1e-3, weight_decay=1e-2)\n",
    "    )\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "batch_size: int = 64\n",
    "max_episode_step: int = 500\n",
    "num_episodes: int = 2000\n",
    "num_warm_up: int = 400"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "when an agent is dead, the only valid action is None",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_3672078/2339975091.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m             \u001b[0mac\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpolicy_nets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mn_agents\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mth\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mob\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m             \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mac\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m             \u001b[0mac_n\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mac\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/juanwu_cs285/lib/python3.7/site-packages/pettingzoo/utils/wrappers/order_enforcing.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     73\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_has_updated\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m             \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mobserve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/juanwu_cs285/lib/python3.7/site-packages/pettingzoo/utils/wrappers/base.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    106\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0magent_selection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0magent_selection\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/juanwu_cs285/lib/python3.7/site-packages/pettingzoo/utils/wrappers/assert_out_of_bounds.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     24\u001b[0m             \u001b[0maction\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         ), \"action is not in action space\"\n\u001b[0;32m---> 26\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__str__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/juanwu_cs285/lib/python3.7/site-packages/pettingzoo/utils/wrappers/base.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    106\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0magent_selection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0magent_selection\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/juanwu_cs285/lib/python3.7/site-packages/pettingzoo/mpe/_mpe_utils/simple_env.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    239\u001b[0m             \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtruncations\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0magent_selection\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    240\u001b[0m         ):\n\u001b[0;32m--> 241\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_was_dead_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    242\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    243\u001b[0m         \u001b[0mcur_agent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0magent_selection\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/juanwu_cs285/lib/python3.7/site-packages/pettingzoo/utils/env.py\u001b[0m in \u001b[0;36m_was_dead_step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    217\u001b[0m         \"\"\"\n\u001b[1;32m    218\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0maction\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 219\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"when an agent is dead, the only valid action is None\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    220\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m         \u001b[0;31m# removes dead agent\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: when an agent is dead, the only valid action is None"
     ]
    }
   ],
   "source": [
    "# Train Loop\n",
    "# =========================================\n",
    "n_agents = len(env.agents)\n",
    "agent_rews = np.empty(shape=(num_episodes, n_agents), dtype='float32')\n",
    "episode_rews = np.empty(shape=(num_episodes, 1), dtype='float32')\n",
    "\n",
    "memory = ReplayBuffer()\n",
    "env_step: int = 0\n",
    "env.reset()\n",
    "\n",
    "for episode in range(num_episodes):\n",
    "    while True:\n",
    "        ob_n, ac_n, rew_n, done_n = [], [], [], []\n",
    "        for i, agent in enumerate(env.agent_iter(max_episode_step * n_agents)):\n",
    "            ob, rew, done, _, info = env.last()\n",
    "            ob_n.append(ob)\n",
    "            rew_n.append(rew)\n",
    "            done_n.append(done)\n",
    "            agent_rews[episode, i % n_agents] += rew\n",
    "\n",
    "            ac = policy_nets[i % n_agents](th.from_numpy(ob).to(device))\n",
    "            env.step(ac.argmax(-1).item())\n",
    "            ac_n.append(ac.detach().cpu().numpy())\n",
    "\n",
    "            memory.add(\n",
    "                obs=np.concatenate(ob_n),\n",
    "                acs=np.concatenate(ac_n),\n",
    "                next_obs=env.state(),\n",
    "                rewards=rew_n,\n",
    "                dones=done_n\n",
    "            )\n",
    "            episode_rews[episode] += np.sum(rew_n)\n",
    "            env_step += int(i % n_agents == 0) \n",
    "\n",
    "            if env_step > num_warm_up and len(memory) > batch_size:\n",
    "                # Update\n",
    "                for i, (policy, tar_policy, critic, tar_critic, p_opt, c_opt) in \\\n",
    "                    enumerate(zip(policy_nets, policy_tar_nets, critic_nets,\n",
    "                                    critic_tar_nets, policy_opts, critic_opts)):\n",
    "                    if c_opt is None:\n",
    "                        continue\n",
    "\n",
    "                    ob_n, ac_n, rew_n, next_ob_n, done_n = memory.sample(\n",
    "                        i, batch_size)\n",
    "                    \n",
    "                    # Update Critic\n",
    "                    pass\n",
    "\n",
    "            done = all(done_n)\n",
    "            if done:\n",
    "                env.reset()\n",
    "    break\n",
    "\n",
    "\n",
    "    # Logging\n",
    "    if env_step > 1 and env_step % 100 == 0:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "76"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(done_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 adversary_0\n",
      "2 agent_0\n",
      "3 agent_1\n",
      "4 adversary_0\n",
      "5 agent_0\n",
      "6 agent_1\n",
      "7 adversary_0\n",
      "8 agent_0\n",
      "9 agent_1\n",
      "10 adversary_0\n",
      "11 agent_0\n",
      "12 agent_1\n",
      "13 adversary_0\n",
      "14 agent_0\n",
      "15 agent_1\n",
      "16 adversary_0\n",
      "17 agent_0\n",
      "18 agent_1\n",
      "19 adversary_0\n",
      "20 agent_0\n",
      "21 agent_1\n",
      "22 adversary_0\n",
      "23 agent_0\n",
      "24 agent_1\n",
      "25 adversary_0\n"
     ]
    }
   ],
   "source": [
    "env.reset()\n",
    "for i, agent in enumerate(env.agent_iter(max_iter=25)):\n",
    "    print(i+1, agent)\n",
    "    env.step(env.action_space(agent).sample())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.13 ('juanwu_cs285')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "dabfb9edd6f0b01bcba20c8b5c22bed4c56635c47168437081608022d6be55db"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
