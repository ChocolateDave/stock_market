{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-Agent Deep Deterministic Policy Gradient for Stock Market"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stock Market Environment\n",
    "\n",
    "- __Hyperparameters__\n",
    "- __Observation Space__\n",
    "  - `stock_price`: `ndarray` of shape $[N_{stock}, ]$\n",
    "  - `correlated_stock`: `ndarray` of shape $[N_{correlated}, ]$\n",
    "  - `uncorrelated_stock`: `ndarray` of shape $[N_{uncorrelated}, ]$\n",
    "  - `budgets`: `ndarray` of shape $[N_{agents}, ]$\n",
    "  - `shares_held`: `ndarray` of shape $[N_{agents}, ]$\n",
    "  - `agent_views`: `ndarray` of shape $[N_{agents}, N_{stock}]$\n",
    "  - `company_states`: `ndarray` of shape $[N_{company}, ]$\n",
    "- __Action Space__\n",
    "  - dimension_1: log buy/sell prices $\\log p\\in\\left(-\\infty, +\\infty\\right)$ => `gym.spaces.Box`\n",
    "  - dimension_2: discrete shares $s\\in\\mathbb{N}$ => `gym.spaces.Discrete`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, Optional, Sequence, Tuple, Union\n",
    "\n",
    "import numpy as np\n",
    "from gym.core import ActType, ObsType, Env\n",
    "from gym.spaces import Box, MultiDiscrete, Tuple as TupleSpace\n",
    "\n",
    "\n",
    "class StockMarketEnv(Env):\n",
    "    \n",
    "    def __init__(self,\n",
    "                 num_agents: int,\n",
    "                 budge_discount: float = 0.9,\n",
    "                 num_company: int = 5,\n",
    "                 num_correlated_stocks: int = 19,\n",
    "                 num_uncorrelated_stocks: int = 10,\n",
    "                 max_shares: int = 100000,\n",
    "                 start_prices: Union[float, Sequence[float]] = 100.0,\n",
    "                 min_budget: float = 100.0,\n",
    "                 max_budget: float = 10000.0,\n",
    "                 budget_discount: float = 0.9,\n",
    "                 step_size: float = 1.0,\n",
    "                 price_std: float = 100.0,\n",
    "                 noise_std: float = 10.0,\n",
    "                 seed: int = 0) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        # Agent Parameters\n",
    "        self.num_agents = num_agents\n",
    "        self.num_company = num_company\n",
    "        self.min_budget = min_budget\n",
    "        self.max_budget = max_budget\n",
    "        self.budget_discount = budge_discount\n",
    "        self.max_shares = max_shares\n",
    "\n",
    "        # Stock Market Parameters\n",
    "        self.dt = step_size\n",
    "        self.start_prices = start_prices\n",
    "        self.price_std = price_std\n",
    "        self.noise_std = noise_std\n",
    "\n",
    "        # Observation and Action spaces\n",
    "        self.n_correlated_stocks = num_correlated_stocks\n",
    "        self.n_uncorrelated_stocks = num_uncorrelated_stocks\n",
    "        self.n_stocks = num_correlated_stocks + num_uncorrelated_stocks + 1\n",
    "        self.observation_space = Box(low=0.0,\n",
    "                                     high=float(\"inf\"),\n",
    "                                     shape=(self.num_agents, self.n_stocks))\n",
    "        self.action_space = TupleSpace(\n",
    "            (Box(low=-float(\"inf\"),\n",
    "                 high=float(\"inf\"),\n",
    "                 shape=(self.num_agents, self.n_stocks)),\n",
    "             MultiDiscrete([[max_shares] * self.n_stocks] * self.num_agents))\n",
    "        )\n",
    "        self._seed = seed\n",
    "\n",
    "    def reset(self,\n",
    "              seed: Optional[int] = None,\n",
    "              return_info: bool = True) -> Tuple[ObsType, Dict]:\n",
    "        self.rng = np.random.default_rng(seed=seed or self._seed)        \n",
    "\n",
    "        correlated_stocks = np.clip(\n",
    "            np.random.normal(loc=self.start_prices,\n",
    "                             scale=self.price_std,\n",
    "                             size=(self.n_correlated_stocks, )),\n",
    "            a_min=1, a_max=None\n",
    "        )\n",
    "        uncorrelated_stocks = np.clip(\n",
    "            np.random.normal(loc=self.start_prices,\n",
    "                             scale=self.price_std,\n",
    "                             size=(self.n_uncorrelated_stocks,)),\n",
    "            a_min=1, a_max=None\n",
    "        )\n",
    "        self.eta = np.clip(\n",
    "            np.random.normal(loc=1.5, scale=1.5, size=(self.num_agents, )),\n",
    "            a_min=0, a_max=10\n",
    "        )\n",
    "        self.valid_mask = np.zeros(shape=(self.num_agents, self.n_stocks),\n",
    "                                   dtype=\"bool\")\n",
    "        self.valid_mask[:, 1:1+self.n_correlated_stocks] = True\n",
    "        self.valid_mask[self.rng.integers(low=0, high=self.num_agents),\n",
    "                        1 + self.n_correlated_stocks:] = True\n",
    "\n",
    "        self.prices = np.asarray(self.start_prices)\n",
    "        self.budgets = self.min_budget + self.rng.random(\n",
    "            size=(self.num_agents), dtype=\"float32\") * (\n",
    "                self.max_budget - self.min_budget)\n",
    "        self.shares = self.rng.integers(low=1,\n",
    "                                        high=self.max_shares,\n",
    "                                        size=(self.num_agents, self.n_stocks))\n",
    "\n",
    "        return (self.prices,\n",
    "                {\n",
    "                    \"correlated_stocks\": correlated_stocks,\n",
    "                    \"uncorrelated_stocks\": uncorrelated_stocks,\n",
    "                    \"budgets\": self.budgets,\n",
    "                    \"shares\": self.shares,\n",
    "                    \"valid_mask\": self.valid_mask,\n",
    "                    \"company_states\": None  # TODO: Company states\n",
    "                })\n",
    "    \n",
    "    def is_terminated(self) -> bool:\n",
    "        return True\n",
    "\n",
    "    def step(self, action: Tuple[np.ndarray, np.ndarray]) -> Tuple:\n",
    "        assert (len(action) == 2 and\n",
    "                action[0].shape == (self.num_agents, self.n_stocks) and\n",
    "                action[1].shape == (self.num_agents, self.n_stocks))\n",
    "        # TODO\n",
    "        proposed_prices = 1. + np.exp(action[0])\n",
    "        proposed_shares = action[1]\n",
    "\n",
    "        # Update budgets and shares\n",
    "        potential_budgets = self.budgets + \\\n",
    "            (proposed_prices * (-proposed_shares)).sum(-1)\n",
    "        potential_shares = self.shares + proposed_shares\n",
    "        print(\"Current budgets: \\n\", potential_budgets,\n",
    "              \"\\nCurrent shares: \\n\", potential_shares)\n",
    "        rewards = np.where(\n",
    "            np.logical_or(potential_budgets < 0.0,\n",
    "                          np.any(potential_shares < 0.0, axis=-1)),\n",
    "            -100, 0.0\n",
    "        )\n",
    "        print(\"Rewards\", rewards)\n",
    "        curr_prices = self.prices\n",
    "        \n",
    "        # TODO\n",
    "\n",
    "\n",
    "        # TODO\n",
    "\n",
    "        dones = self.is_terminated()\n",
    "        if dones:\n",
    "            next_s, _ = self.reset()\n",
    "\n",
    "        return \n",
    "\n",
    "    @staticmethod\n",
    "    def utility(c: float, eta: float) -> float:\n",
    "        if eta!= 1.0:\n",
    "            return (c ** (1.0 - eta) - 1.0) / (1.0 - eta)\n",
    "        else:\n",
    "            return np.log(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current budgets: \n",
      " [-27991.42362094 -34178.41267729 -35891.77252471 -37679.80823529\n",
      " -28668.28449202 -30585.09359443 -38112.77494562 -39650.0941211\n",
      " -34963.18385458 -40977.61369228] \n",
      "Current shares: \n",
      " [[1484 1084  874 1722  790  738  632  981 1233  964 1773 1594  170  739\n",
      "  1737 1070  954  836 1193 1766  763  630 1780  100  614  867  905  656\n",
      "   694  837]\n",
      " [ 901   65  285  760  834  718 1305 1028  896 1513 1300  732 1520 1772\n",
      "  1731  940  780 1930 1643 1055  899 1620  485 1600 1024 1432 1497  851\n",
      "   842 1002]\n",
      " [ 680  979 1288 1587 1220  662 1828  627 1331 1392  657  706 1110 1577\n",
      "   886 1109  394  844 1159  640 1636  907  771 1453 1416   84  534  670\n",
      "  1154 1186]\n",
      " [1769  664 1006  804 1715  909  749 1240  650 1330  673 1422 1099 1214\n",
      "  1009 1681  948  973  696  943  891 1376  603 1633  437 1555 1536 1478\n",
      "   885 1355]\n",
      " [1624 1135  920 1581 1328  710  748 1583  378  787  974  853  995 1090\n",
      "   681  976 1628  836  930  425 1392 1659 1175  384 1946  692 1394   18\n",
      "   644 1181]\n",
      " [1590 1305  886 1170 1428  606 1206  288  816 1016  976 1477  924 1459\n",
      "  1270  369 1060  913 1339  970 1669 1053 1552 1084  886  711  939  272\n",
      "  1023 1632]\n",
      " [1788  512  786 1009 1707  954 1411  886 1050  237  735  358  645 1041\n",
      "  1046  763 1554 1522  580 1238  762 1037  538  482 1283 1683  329  827\n",
      "   832  825]\n",
      " [ 713 1218  321 1319 1513 1039  106  428 1119  965  834 1046 1517  615\n",
      "   785 1418 1382 1453  731 1515 1308 1132  466 1492 1745 1052 1413 1402\n",
      "   908  397]\n",
      " [ 306 1140 1621  773  571  757  376  961  972 1670 1388 1837 1123 1391\n",
      "  1140  493  773 1320  791 1423  558  676 1310 1633 1136 1164 1564 1167\n",
      "  1164  282]\n",
      " [1470  827  597  125 1685 1603  263 1625  415  301 1712 1830  636  871\n",
      "  1303 1448 1249 1125 1200  522  826 1056 1144  988  798  972 1607  945\n",
      "   630 1488]]\n",
      "Rewards [-100. -100. -100. -100. -100. -100. -100. -100. -100. -100.]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(None, None)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = StockMarketEnv(10)\n",
    "env.reset()\n",
    "random_action = env.action_space.sample()\n",
    "\n",
    "env.step(random_action)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MADDPG Trainer\n",
    "\n",
    "The `MADDPG Trainer` class is a generic version of the `DDPG` trainer initialized with\n",
    "- A sequence of `DDPG Agent` class objects\n",
    "- A shared observation buffer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DDPG Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import torch as th\n",
    "from gym.core import Env\n",
    "from gym.spaces import Discrete\n",
    "from src.agent.ddpg_agent import DDPGAgent\n",
    "from src.critic.ddpg_critic import DDPGCritic\n",
    "from src.memory.base_buffer import Path\n",
    "from src.memory.replay_buffer import ReplayBuffer\n",
    "from src.policy.ddpg_policy import DDPGPolicy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8,) 4\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"LunarLander-v2\", new_step_api=True)\n",
    "print(env.observation_space.shape, env.action_space.n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "if isinstance(env.action_space, Discrete):\n",
    "    action_size = 1\n",
    "else:\n",
    "    action_size = env.action_space.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = DDPGAgent(\n",
    "    observation_size=env.observation_space.shape[0],\n",
    "    action_size=action_size,\n",
    "    policy_net=\"MLP\",\n",
    "    policy_net_kwargs={\"hidden_size\": 64, \"num_layers\": 2},\n",
    "    critic_net=\"MLP\",\n",
    "    critic_net_kwargs={\"hidden_size\": 64, \"num_layers\": 2}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "replay_buffer = ReplayBuffer(max_size=1000)\n",
    "while len(replay_buffer) < 1000:\n",
    "    obs, acs, next_obs, rews, dones = [], [], [], [], []\n",
    "    ob = env.reset()\n",
    "    done = False\n",
    "    while not done:\n",
    "        ac = env.action_space.sample()\n",
    "        next_ob, rew, done, _, _ = env.step(ac)\n",
    "        \n",
    "        obs.append(ob)\n",
    "        acs.append(ac)\n",
    "        next_obs.append(next_ob)\n",
    "        rews.append(rew)\n",
    "        dones.append(done)\n",
    "    \n",
    "    path = Path(observation=np.asarray(obs, dtype=\"float32\"),\n",
    "                action=np.asarray(acs, dtype=\"float32\"),\n",
    "                next_observation=np.asarray(next_obs, dtype=\"float32\"),\n",
    "                reward=np.asarray(rews, dtype=\"float32\"),\n",
    "                done=np.asarray(dones, dtype=\"int64\"))\n",
    "    replay_buffer.add([path], noised=False)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([244, 1]) torch.Size([244, 244])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/anaconda3/envs/cs285/lib/python3.7/site-packages/torch/nn/modules/loss.py:530: UserWarning: Using a target size (torch.Size([244, 244])) that is different to the input size (torch.Size([244, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    }
   ],
   "source": [
    "# Main Training Loop\n",
    "obs, acs, next_obs, rews, dones = replay_buffer.sample(64)\n",
    "agent.train_one_step(obs, acs, next_obs, rews, dones)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(244,)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obs = th.from_numpy(obs)\n",
    "acs = th.from_numpy(acs)\n",
    "next_obs = th.from_numpy(next_obs)\n",
    "rews = th.from_numpy(rews)\n",
    "dones = th.from_numpy(dones)\n",
    "\n",
    "next_acs = agent.policy.get_action(next_obs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation\n",
    "\n",
    "For validation of the reproducing results, we run the trainer on the [Multi-Agent Partical Environment](https://github.com/openai/multiagent-particle-envs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pettingzoo.mpe import simple_adversary_v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Box(-inf, inf, (8,), float32)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = simple_adversary_v2.env(N=2, max_cycles=25, continuous_actions=False)\n",
    "env.reset(seed=42)\n",
    "env.observation_space('adversary_0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_space('adversary_0').sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.69437176, -1.4609686 ,  0.023015  , -0.97559977,  0.51697916,\n",
       "       -1.5288411 ,  1.0734879 , -0.19491644], dtype=float32)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.observe(env.agents[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.13 ('cs285')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "154a4bcdb547ae2ed2207f73ecf8d8082d2df07b3c4e420b64ca06f8780133a6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
