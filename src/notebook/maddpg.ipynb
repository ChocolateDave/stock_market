{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-Agent Deep Deterministic Policy Gradient for Stock Market"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stock Market Environment\n",
    "\n",
    "- __Hyperparameters__\n",
    "- __Observation Space__\n",
    "  - `stock_price`: `ndarray` of shape $[N_{stock}, ]$\n",
    "  - `correlated_stock`: `ndarray` of shape $[N_{correlated}, ]$\n",
    "  - `uncorrelated_stock`: `ndarray` of shape $[N_{uncorrelated}, ]$\n",
    "  - `budgets`: `ndarray` of shape $[N_{agents}, ]$\n",
    "  - `shares_held`: `ndarray` of shape $[N_{agents}, ]$\n",
    "  - `agent_views`: `ndarray` of shape $[N_{agents}, N_{stock}]$\n",
    "  - `company_states`: `ndarray` of shape $[N_{company}, ]$\n",
    "- __Action Space__\n",
    "  - dimension_1: log buy/sell prices $\\log p\\in\\left(-\\infty, +\\infty\\right)$ => `gym.spaces.Box`\n",
    "  - dimension_2: discrete shares $s\\in\\mathbb{N}$ => `gym.spaces.Discrete`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, Optional, Sequence, Tuple, Union\n",
    "\n",
    "import numpy as np\n",
    "from gym.core import ActType, ObsType, Env\n",
    "from gym.spaces import Box, MultiDiscrete, Tuple as TupleSpace\n",
    "\n",
    "\n",
    "class StockMarketEnv(Env):\n",
    "    \n",
    "    def __init__(self,\n",
    "                 num_agents: int,\n",
    "                 budget_discount: float = 0.9,\n",
    "                 num_company: int = 5,\n",
    "                 num_correlated_stocks: int = 19,\n",
    "                 num_uncorrelated_stocks: int = 10,\n",
    "                 max_shares: int = 100000,\n",
    "                 start_prices: Union[float, Sequence[float]] = 100.0,\n",
    "                 min_budget: float = 100.0,\n",
    "                 max_budget: float = 10000.0,\n",
    "                 budget_discount: float = 0.9,\n",
    "                 step_size: float = 1.0,\n",
    "                 price_std: float = 100.0,\n",
    "                 noise_std: float = 10.0,\n",
    "                 seed: int = 0) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        # Agent Parameters\n",
    "        self.num_agents = num_agents\n",
    "        self.num_company = num_company\n",
    "        self.min_budget = min_budget\n",
    "        self.max_budget = max_budget\n",
    "        self.budget_discount = budget_discount\n",
    "        self.max_shares = max_shares\n",
    "\n",
    "        # Stock Market Parameters\n",
    "        self.dt = step_size\n",
    "        self.start_prices = start_prices\n",
    "        self.price_std = price_std\n",
    "        self.noise_std = noise_std\n",
    "\n",
    "        # Observation and Action spaces\n",
    "        self.n_correlated_stocks = num_correlated_stocks\n",
    "        self.n_uncorrelated_stocks = num_uncorrelated_stocks\n",
    "        self.n_stocks = num_correlated_stocks + num_uncorrelated_stocks + 1\n",
    "        self.observation_space = Box(low=0.0,\n",
    "                                     high=float(\"inf\"),\n",
    "                                     shape=(self.num_agents, self.n_stocks))\n",
    "        self.action_space = TupleSpace(\n",
    "            (Box(low=-float(\"inf\"),\n",
    "                 high=float(\"inf\"),\n",
    "                 shape=(self.num_agents, self.n_stocks)),\n",
    "             MultiDiscrete([[max_shares] * self.n_stocks] * self.num_agents))\n",
    "        )\n",
    "        self._seed = seed\n",
    "\n",
    "    def reset(self,\n",
    "              seed: Optional[int] = None,\n",
    "              return_info: bool = True) -> Tuple[ObsType, Dict]:\n",
    "        self.rng = np.random.default_rng(seed=seed or self._seed)        \n",
    "\n",
    "        correlated_stocks = np.clip(\n",
    "            np.random.normal(loc=self.start_prices,\n",
    "                             scale=self.price_std,\n",
    "                             size=(self.n_correlated_stocks, )),\n",
    "            a_min=1, a_max=None\n",
    "        )\n",
    "        uncorrelated_stocks = np.clip(\n",
    "            np.random.normal(loc=self.start_prices,\n",
    "                             scale=self.price_std,\n",
    "                             size=(self.n_uncorrelated_stocks,)),\n",
    "            a_min=1, a_max=None\n",
    "        )\n",
    "        self.eta = np.clip(\n",
    "            np.random.normal(loc=1.5, scale=1.5, size=(self.num_agents, )),\n",
    "            a_min=0, a_max=10\n",
    "        )\n",
    "        self.valid_mask = np.zeros(shape=(self.num_agents, self.n_stocks),\n",
    "                                   dtype=\"bool\")\n",
    "        self.valid_mask[:, 1:1+self.n_correlated_stocks] = True\n",
    "        self.valid_mask[self.rng.integers(low=0, high=self.num_agents),\n",
    "                        1 + self.n_correlated_stocks:] = True\n",
    "\n",
    "        self.prices = np.asarray(self.start_prices)\n",
    "        self.budgets = self.min_budget + self.rng.random(\n",
    "            size=(self.num_agents), dtype=\"float32\") * (\n",
    "                self.max_budget - self.min_budget)\n",
    "        self.shares = self.rng.integers(low=1,\n",
    "                                        high=self.max_shares,\n",
    "                                        size=(self.num_agents, self.n_stocks))\n",
    "\n",
    "        return (self.prices,\n",
    "                {\n",
    "                    \"correlated_stocks\": correlated_stocks,\n",
    "                    \"uncorrelated_stocks\": uncorrelated_stocks,\n",
    "                    \"budgets\": self.budgets,\n",
    "                    \"shares\": self.shares,\n",
    "                    \"valid_mask\": self.valid_mask,\n",
    "                    \"company_states\": None  # TODO: Company states\n",
    "                })\n",
    "    \n",
    "    def is_terminated(self) -> bool:\n",
    "        return True\n",
    "\n",
    "    def step(self, action: Tuple[np.ndarray, np.ndarray]) -> Tuple:\n",
    "        assert (len(action) == 2 and\n",
    "                action[0].shape == (self.num_agents, self.n_stocks) and\n",
    "                action[1].shape == (self.num_agents, self.n_stocks))\n",
    "        # TODO\n",
    "        proposed_prices = 1. + np.exp(action[0])\n",
    "        proposed_shares = action[1]\n",
    "\n",
    "        # Update budgets and shares\n",
    "        potential_budgets = self.budgets + \\\n",
    "            (proposed_prices * (-proposed_shares)).sum(-1)\n",
    "        potential_shares = self.shares + proposed_shares\n",
    "        print(\"Current budgets: \\n\", potential_budgets,\n",
    "              \"\\nCurrent shares: \\n\", potential_shares)\n",
    "        rewards = np.where(\n",
    "            np.logical_or(potential_budgets < 0.0,\n",
    "                          np.any(potential_shares < 0.0, axis=-1)),\n",
    "            -100, 0.0\n",
    "        )\n",
    "        print(\"Rewards\", rewards)\n",
    "        curr_prices = self.prices\n",
    "        \n",
    "        # TODO\n",
    "\n",
    "\n",
    "        # TODO\n",
    "\n",
    "        dones = self.is_terminated()\n",
    "        if dones:\n",
    "            next_s, _ = self.reset()\n",
    "\n",
    "        return \n",
    "\n",
    "    @staticmethod\n",
    "    def utility(c: float, eta: float) -> float:\n",
    "        if eta!= 1.0:\n",
    "            return (c ** (1.0 - eta) - 1.0) / (1.0 - eta)\n",
    "        else:\n",
    "            return np.log(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current budgets: \n",
      " [-27991.42362094 -34178.41267729 -35891.77252471 -37679.80823529\n",
      " -28668.28449202 -30585.09359443 -38112.77494562 -39650.0941211\n",
      " -34963.18385458 -40977.61369228] \n",
      "Current shares: \n",
      " [[1484 1084  874 1722  790  738  632  981 1233  964 1773 1594  170  739\n",
      "  1737 1070  954  836 1193 1766  763  630 1780  100  614  867  905  656\n",
      "   694  837]\n",
      " [ 901   65  285  760  834  718 1305 1028  896 1513 1300  732 1520 1772\n",
      "  1731  940  780 1930 1643 1055  899 1620  485 1600 1024 1432 1497  851\n",
      "   842 1002]\n",
      " [ 680  979 1288 1587 1220  662 1828  627 1331 1392  657  706 1110 1577\n",
      "   886 1109  394  844 1159  640 1636  907  771 1453 1416   84  534  670\n",
      "  1154 1186]\n",
      " [1769  664 1006  804 1715  909  749 1240  650 1330  673 1422 1099 1214\n",
      "  1009 1681  948  973  696  943  891 1376  603 1633  437 1555 1536 1478\n",
      "   885 1355]\n",
      " [1624 1135  920 1581 1328  710  748 1583  378  787  974  853  995 1090\n",
      "   681  976 1628  836  930  425 1392 1659 1175  384 1946  692 1394   18\n",
      "   644 1181]\n",
      " [1590 1305  886 1170 1428  606 1206  288  816 1016  976 1477  924 1459\n",
      "  1270  369 1060  913 1339  970 1669 1053 1552 1084  886  711  939  272\n",
      "  1023 1632]\n",
      " [1788  512  786 1009 1707  954 1411  886 1050  237  735  358  645 1041\n",
      "  1046  763 1554 1522  580 1238  762 1037  538  482 1283 1683  329  827\n",
      "   832  825]\n",
      " [ 713 1218  321 1319 1513 1039  106  428 1119  965  834 1046 1517  615\n",
      "   785 1418 1382 1453  731 1515 1308 1132  466 1492 1745 1052 1413 1402\n",
      "   908  397]\n",
      " [ 306 1140 1621  773  571  757  376  961  972 1670 1388 1837 1123 1391\n",
      "  1140  493  773 1320  791 1423  558  676 1310 1633 1136 1164 1564 1167\n",
      "  1164  282]\n",
      " [1470  827  597  125 1685 1603  263 1625  415  301 1712 1830  636  871\n",
      "  1303 1448 1249 1125 1200  522  826 1056 1144  988  798  972 1607  945\n",
      "   630 1488]]\n",
      "Rewards [-100. -100. -100. -100. -100. -100. -100. -100. -100. -100.]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(None, None)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = StockMarketEnv(10)\n",
    "env.reset()\n",
    "random_action = env.action_space.sample()\n",
    "\n",
    "env.step(random_action)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MADDPG Trainer\n",
    "\n",
    "The `MADDPG Trainer` class is a generic version of the `DDPG` trainer initialized with\n",
    "- A sequence of `DDPG Agent` class objects\n",
    "- A shared observation buffer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "from copy import deepcopy\n",
    "from typing import Any, Optional, Sequence, Tuple, Union\n",
    "\n",
    "import numpy as np\n",
    "import torch as th\n",
    "from numpy import ndarray\n",
    "from pettingzoo.mpe import simple_adversary_v2\n",
    "from src.memory.multi_replay_buffer import MultiAgentReplayBuffer\n",
    "from torch import Tensor, nn, optim\n",
    "from torch.nn import functional as F\n",
    "\n",
    "device = th.device('cuda:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model\n",
    "class PolicyNet(nn.Module):\n",
    "\n",
    "    def __init__(self,\n",
    "                 in_features: int,\n",
    "                 action_size: int,\n",
    "                 num_hidden_1: int = 400,\n",
    "                 num_hidden_2: int = 300,\n",
    "                 negative_slope: float = 0.01) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.linear_1 = nn.Linear(in_features, num_hidden_1)\n",
    "        self.linear_2 = nn.Linear(num_hidden_1, num_hidden_2)\n",
    "        self.linear_3 = nn.Linear(num_hidden_2, action_size)\n",
    "        self.neg_slope = negative_slope\n",
    "\n",
    "        self.reset_parameters()\n",
    "    \n",
    "    def forward(self, obs: Tensor) -> Tensor:\n",
    "        obs = obs.float()\n",
    "        obs = F.leaky_relu(self.linear_1(obs), self.neg_slope)\n",
    "        obs = F.leaky_relu(self.linear_2(obs), self.neg_slope)\n",
    "        acs = th.tanh(self.linear_3(obs))\n",
    "        \n",
    "        return acs\n",
    "    \n",
    "    def reset_parameters(self) -> None:\n",
    "        gain_lrelu = nn.init.calculate_gain('leaky_relu')\n",
    "        gain_tanh = nn.init.calculate_gain('tanh')\n",
    "        nn.init.xavier_uniform_(self.linear_1.weight, gain=gain_lrelu)\n",
    "        nn.init.xavier_uniform_(self.linear_2.weight, gain=gain_lrelu)\n",
    "        nn.init.xavier_uniform_(self.linear_3.weight, gain=gain_tanh)\n",
    "\n",
    "\n",
    "class CriticNet(nn.Module):\n",
    "    \n",
    "    def __init__(self,\n",
    "                 obs_in_features: int,\n",
    "                 acs_in_features: int,\n",
    "                 num_hidden_1: int = 400,\n",
    "                 num_hidden_2: int = 300,\n",
    "                 negative_slope: float = 0.01) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.linear_1 = nn.Linear(obs_in_features, num_hidden_1)\n",
    "        self.linear_2 = nn.Linear(num_hidden_1 + acs_in_features, num_hidden_2)\n",
    "        self.linear_3 = nn.Linear(num_hidden_2, 1)\n",
    "        self.neg_slope = negative_slope\n",
    "\n",
    "    def forward(self, obs: Tensor, acs: Tensor) -> Tensor:\n",
    "        obs = obs.float()\n",
    "        acs = acs.float()\n",
    "\n",
    "        obs = F.leaky_relu(self.linear_1(obs), self.neg_slope)\n",
    "        q_val = F.leaky_relu(self.linear_2(th.cat([obs, acs], -1)),\n",
    "                             self.neg_slope)\n",
    "        q_val = self.linear_3(q_val)\n",
    "\n",
    "        return q_val\n",
    "    \n",
    "    def reset_parameters(self) -> None:\n",
    "        gain = nn.init.calculate_gain('leaky_relu')\n",
    "        nn.init.xavier_uniform_(self.linear_1.weight, gain)\n",
    "        nn.init.xavier_uniform_(self.linear_2.weight, gain)\n",
    "        nn.init.xavier_uniform_(self.linear_3.weight, gain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = simple_adversary_v2.parallel_env(max_cycles=25)\n",
    "env.reset()\n",
    "# Initialize agents\n",
    "def hard_update(src: nn.Module,\n",
    "                tar: nn.Module,\n",
    "                non_blocking: bool = True) -> None:\n",
    "    with th.no_grad():\n",
    "        for param, tar_param in zip(src.parameters(), tar.parameters()):\n",
    "            param.data.copy_(tar_param, non_blocking)\n",
    "\n",
    "def soft_update(src: nn.Module,\n",
    "                tar: nn.Module,\n",
    "                tau: float = 0.001,\n",
    "                non_blocking: bool = True) -> None:\n",
    "    with th.no_grad():\n",
    "        for param, tar_param in zip(src.parameters(), tar.parameters()):\n",
    "            param.data.copy_(param * tau + tar_param * (1 - tau))\n",
    "\n",
    "policy_nets, critic_nets = {}, {} \n",
    "policy_tar_nets, critic_tar_nets = {}, {}\n",
    "policy_opts, critic_opts = {}, {}\n",
    "global_obs_size, global_acs_size = 0, 0\n",
    "for agent in env.agents:\n",
    "    if len(env.observation_space(agent).shape) > 2:\n",
    "        raise RuntimeError('Image inputs not supported')\n",
    "    global_obs_size += env.observation_space(agent).shape[0]\n",
    "    global_acs_size += env.action_space(agent).n\n",
    "\n",
    "for agent in env.agents:\n",
    "    policy_nets[agent] = PolicyNet(env.observation_space(agent).shape[0],\n",
    "                                   env.action_space(agent).n).to(device)\n",
    "    policy_tar_nets[agent] = PolicyNet(env.observation_space(agent).shape[0],\n",
    "                                       env.action_space(agent).n).to(device)\n",
    "    hard_update(policy_tar_nets[agent], policy_nets[agent])\n",
    "    critic_nets[agent] = \\\n",
    "        CriticNet(global_obs_size, global_acs_size).to(device)\n",
    "    critic_tar_nets[agent] = \\\n",
    "        CriticNet(global_obs_size, global_acs_size).to(device)\n",
    "    hard_update(critic_tar_nets[agent], critic_nets[agent])\n",
    "    policy_opts[agent] = optim.Adam(policy_nets[agent].parameters(), lr=1e-4)\n",
    "    critic_opts[agent] = \\\n",
    "        optim.Adam(critic_nets[agent].parameters(), lr=1e-3, weight_decay=1e-2)\n",
    "\n",
    "buffer = MultiAgentReplayBuffer(env.agents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "batch_size: int = 64\n",
    "discount: float = 0.99\n",
    "max_episode_step: int = 500\n",
    "num_episodes: int = 2000\n",
    "num_warm_up: int = 400"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_2535443/2873209878.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     86\u001b[0m                 aloss = -critic_nets[_id].forward(\n\u001b[1;32m     87\u001b[0m                     states, th.hstack(list(acs_n.values())))\n\u001b[0;32m---> 88\u001b[0;31m                 \u001b[0mpolicy_opts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0m_id\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/juanwu_cs285/lib/python3.7/site-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    111\u001b[0m                 \u001b[0mprofile_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Optimizer.step#{}.step\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecord_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprofile_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    114\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/juanwu_cs285/lib/python3.7/site-packages/torch/autograd/grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/juanwu_cs285/lib/python3.7/site-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    169\u001b[0m                  \u001b[0mmaximize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'maximize'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m                  \u001b[0mforeach\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'foreach'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 171\u001b[0;31m                  capturable=group['capturable'])\n\u001b[0m\u001b[1;32m    172\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/juanwu_cs285/lib/python3.7/site-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36madam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[1;32m    224\u001b[0m          \u001b[0meps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0meps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    225\u001b[0m          \u001b[0mmaximize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmaximize\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 226\u001b[0;31m          capturable=capturable)\n\u001b[0m\u001b[1;32m    227\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    228\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/juanwu_cs285/lib/python3.7/site-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36m_single_tensor_adam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize, capturable)\u001b[0m\n\u001b[1;32m    305\u001b[0m                 \u001b[0mdenom\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mexp_avg_sq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mbias_correction2_sqrt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 307\u001b[0;31m             \u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddcdiv_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexp_avg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdenom\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mstep_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    308\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    309\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Train Loop\n",
    "# =========================================\n",
    "n_agents = len(env.agents)\n",
    "agent_rews = np.empty(shape=(num_episodes, n_agents), dtype='float32')\n",
    "episode_rews = np.empty(shape=(num_episodes, 1), dtype='float32')\n",
    "env_step: int = 0\n",
    "\n",
    "def to_one_hot(data: int, num_classes: int = -1) -> np.ndarray:\n",
    "    if num_classes == -1:\n",
    "        num_classes = int(max(data) + 1)\n",
    "\n",
    "    if isinstance(data, int):\n",
    "        output = np.zeros(shape=(num_classes,))\n",
    "        output[data] = 1\n",
    "    else:\n",
    "        output = data\n",
    "\n",
    "    return output\n",
    "\n",
    "    \n",
    "for episode in range(num_episodes):\n",
    "    ob_n = env.reset()\n",
    "    while env.agents:\n",
    "        env_step += 1\n",
    "        if env_step <= num_warm_up:\n",
    "            actions = {_a: env.action_space(_a).sample() for _a in env.agents}\n",
    "            ac_n = {_a: to_one_hot(ac, env.action_space(_a).n)\n",
    "                    for _a, ac in actions.items()}\n",
    "        else:\n",
    "            actions, ac_n = {}, {}\n",
    "            for agent, ob in ob_n.items():\n",
    "                ob = th.from_numpy(ob).view(1, -1).float().to(device)\n",
    "                ac = F.gumbel_softmax(policy_nets[agent].forward(ob))\n",
    "                actions[agent] = ac.view(-1).argmax().item()\n",
    "                ac_n[agent] = ac.detach().cpu().numpy()\n",
    "            \n",
    "        next_ob_n, rew_n, done_n, _, _ = env.step(actions)\n",
    "        buffer.add_transition(ob_n, ac_n, next_ob_n, rew_n, done_n)\n",
    "\n",
    "        # Learn\n",
    "        if len(buffer) > batch_size:\n",
    "            for _id in env.agents:\n",
    "                obs_n, acs_n, next_obs_n, rew_n, dones_n = \\\n",
    "                    buffer.sample(batch_size, random=True, device=device) \n",
    "\n",
    "                # Centralized observation\n",
    "                states = th.hstack(list(obs_n.values()))\n",
    "                next_states = th.hstack(list(next_obs_n.values()))\n",
    "                actions = th.hstack(list(acs_n.values()))\n",
    "                next_actions = th.hstack(\n",
    "                    [F.gumbel_softmax(\n",
    "                        policy_tar_nets[_id].forward(next_obs_n[_id]),\n",
    "                        hard=True\n",
    "                    ).detach()\n",
    "                            for _id in env.agents]\n",
    "                )\n",
    "\n",
    "            \n",
    "                ob, ac, next_ob, rew, done = (\n",
    "                    obs_n[_id],\n",
    "                    acs_n[_id],\n",
    "                    next_ob_n[_id],\n",
    "                    rew_n[_id],\n",
    "                    dones_n[_id]\n",
    "                )                \n",
    "\n",
    "                # Update critic network\n",
    "                rew = rew.view(-1, 1)\n",
    "                done = done.view(-1, 1).to(device)\n",
    "                actions = th.hstack(list(acs_n.values())).to(device)\n",
    "                q_val = critic_nets[_id].forward(states, actions)\n",
    "                tar_q_val = rew + discount * (1 - done) * \\\n",
    "                    critic_tar_nets[_id](next_states, next_actions)\n",
    "                critic_opts[_id].zero_grad()\n",
    "                closs = F.mse_loss(q_val, tar_q_val.detach(), reduction='mean')\n",
    "                closs.backward()\n",
    "                critic_opts[_id].step()\n",
    "\n",
    "                # Update policy network\n",
    "                ob = ob.to(device)\n",
    "                new_logits = policy_nets[_id].forward(ob)\n",
    "                new_action = F.gumbel_softmax(new_logits, hard=True)\n",
    "                acs_n[_id] = new_action\n",
    "                policy_opts[_id].zero_grad()\n",
    "                aloss = -critic_nets[_id].forward(\n",
    "                    states, th.hstack(list(acs_n.values())))\n",
    "                policy_opts[_id].step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ob' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_2535443/3624012301.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpolicy_nets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'agent_0'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mth\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mob\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgumbel_softmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhard\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'ob' is not defined"
     ]
    }
   ],
   "source": [
    "logits = policy_nets['agent_0'].forward(th.from_numpy(ob).to(device))\n",
    "F.gumbel_softmax(logits, hard=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "th.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stock Market Evaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.environment.stock_market import (\n",
    "    LogarithmAndIntActionWrapper, StockMarketEnv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'agent_0': array([  0.        ,  98.11713208,  46.73370522,   1.        ,\n",
       "         82.78770243, 153.09555082, 211.81191222,  74.52338782,\n",
       "          1.        , 166.1772597 ,  29.72760639,  31.73528957,\n",
       "        168.5467816 , 124.80253407, 117.71429121, 187.30074633,\n",
       "        131.34482528,  64.78192649, 174.03601525, 337.67939808,\n",
       "        110.22890678, 155.31159852,  10.07443286, 171.23993985,\n",
       "         90.62127185,   1.        , 150.7712587 , 169.39050963,\n",
       "        263.788417  , 207.89874706]),\n",
       " 'agent_1': array([  0.        ,  98.11713208,  46.73370522,   1.        ,\n",
       "         82.78770243, 153.09555082, 211.81191222,  74.52338782,\n",
       "          1.        , 166.1772597 ,  29.72760639,  31.73528957,\n",
       "        168.5467816 , 124.80253407, 117.71429121, 187.30074633,\n",
       "        131.34482528,  64.78192649, 174.03601525, 337.67939808,\n",
       "        110.22890678, 155.31159852,  10.07443286, 171.23993985,\n",
       "         90.62127185,   1.        , 150.7712587 , 169.39050963,\n",
       "        263.788417  , 207.89874706]),\n",
       " 'agent_2': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " 'agent_3': array([  0.        ,  98.11713208,  46.73370522,   1.        ,\n",
       "         82.78770243, 153.09555082, 211.81191222,  74.52338782,\n",
       "          1.        , 166.1772597 ,  29.72760639,  31.73528957,\n",
       "        168.5467816 , 124.80253407, 117.71429121, 187.30074633,\n",
       "        131.34482528,  64.78192649, 174.03601525, 337.67939808,\n",
       "        110.22890678, 155.31159852,  10.07443286, 171.23993985,\n",
       "         90.62127185,   1.        , 150.7712587 , 169.39050963,\n",
       "        263.788417  , 207.89874706]),\n",
       " 'agent_4': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = LogarithmAndIntActionWrapper(\n",
    "    StockMarketEnv(num_agents=5))\n",
    "env.reset()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.13 ('juanwu_cs285')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "dabfb9edd6f0b01bcba20c8b5c22bed4c56635c47168437081608022d6be55db"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
