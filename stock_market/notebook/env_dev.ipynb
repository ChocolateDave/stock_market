{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-Agent Deep Deterministic Policy Gradient for Stock Market"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stock Market Environment\n",
    "\n",
    "- __Hyperparameters__\n",
    "- __Observation Space__\n",
    "  - `stock_price`: `ndarray` of shape $[N_{stock}, ]$\n",
    "  - `correlated_stock`: `ndarray` of shape $[N_{correlated}, ]$\n",
    "  - `uncorrelated_stock`: `ndarray` of shape $[N_{uncorrelated}, ]$\n",
    "  - `budgets`: `ndarray` of shape $[N_{agents}, ]$\n",
    "  - `shares_held`: `ndarray` of shape $[N_{agents}, ]$\n",
    "  - `agent_views`: `ndarray` of shape $[N_{agents}, N_{stock}]$\n",
    "  - `company_states`: `ndarray` of shape $[N_{company}, ]$\n",
    "- __Action Space__\n",
    "  - dimension_1: log buy/sell prices $\\log p\\in\\left(-\\infty, +\\infty\\right)$ => `gym.spaces.Box`\n",
    "  - dimension_2: discrete shares $s\\in\\mathbb{N}$ => `gym.spaces.Discrete`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, Optional, Sequence, Tuple, Union\n",
    "\n",
    "import numpy as np\n",
    "from gym.core import ActType, ObsType, Env\n",
    "from gym.spaces import Box, MultiDiscrete, Tuple as TupleSpace\n",
    "\n",
    "\n",
    "class StockMarketEnv(Env):\n",
    "    \n",
    "    def __init__(self,\n",
    "                 num_agents: int,\n",
    "                 budget_discount: float = 0.9,\n",
    "                 num_company: int = 5,\n",
    "                 num_correlated_stocks: int = 19,\n",
    "                 num_uncorrelated_stocks: int = 10,\n",
    "                 max_shares: int = 100000,\n",
    "                 start_prices: Union[float, Sequence[float]] = 100.0,\n",
    "                 min_budget: float = 100.0,\n",
    "                 max_budget: float = 10000.0,\n",
    "                 step_size: float = 1.0,\n",
    "                 price_std: float = 100.0,\n",
    "                 noise_std: float = 10.0,\n",
    "                 seed: int = 0) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        # Agent Parameters\n",
    "        self.num_agents = num_agents\n",
    "        self.num_company = num_company\n",
    "        self.min_budget = min_budget\n",
    "        self.max_budget = max_budget\n",
    "        self.budget_discount = budget_discount\n",
    "        self.max_shares = max_shares\n",
    "\n",
    "        # Stock Market Parameters\n",
    "        self.dt = step_size\n",
    "        self.start_prices = start_prices\n",
    "        self.price_std = price_std\n",
    "        self.noise_std = noise_std\n",
    "\n",
    "        # Observation and Action spaces\n",
    "        self.n_correlated_stocks = num_correlated_stocks\n",
    "        self.n_uncorrelated_stocks = num_uncorrelated_stocks\n",
    "        self.n_stocks = num_correlated_stocks + num_uncorrelated_stocks + 1\n",
    "        self.observation_space = Box(low=0.0,\n",
    "                                     high=float(\"inf\"),\n",
    "                                     shape=(self.num_agents, self.n_stocks))\n",
    "        self.action_space = TupleSpace(\n",
    "            (Box(low=-float(\"inf\"),\n",
    "                 high=float(\"inf\"),\n",
    "                 shape=(self.num_agents, self.n_stocks)),\n",
    "             MultiDiscrete([[max_shares] * self.n_stocks] * self.num_agents))\n",
    "        )\n",
    "        self._seed = seed\n",
    "\n",
    "    def reset(self,\n",
    "              seed: Optional[int] = None,\n",
    "              return_info: bool = True) -> Tuple[ObsType, Dict]:\n",
    "        self.rng = np.random.default_rng(seed=seed or self._seed)        \n",
    "\n",
    "        correlated_stocks = np.clip(\n",
    "            np.random.normal(loc=self.start_prices,\n",
    "                             scale=self.price_std,\n",
    "                             size=(self.n_correlated_stocks, )),\n",
    "            a_min=1, a_max=None\n",
    "        )\n",
    "        uncorrelated_stocks = np.clip(\n",
    "            np.random.normal(loc=self.start_prices,\n",
    "                             scale=self.price_std,\n",
    "                             size=(self.n_uncorrelated_stocks,)),\n",
    "            a_min=1, a_max=None\n",
    "        )\n",
    "        self.eta = np.clip(\n",
    "            np.random.normal(loc=1.5, scale=1.5, size=(self.num_agents, )),\n",
    "            a_min=0, a_max=10\n",
    "        )\n",
    "        self.valid_mask = np.zeros(shape=(self.num_agents, self.n_stocks),\n",
    "                                   dtype=\"bool\")\n",
    "        self.valid_mask[:, 1:1+self.n_correlated_stocks] = True\n",
    "        self.valid_mask[self.rng.integers(low=0, high=self.num_agents),\n",
    "                        1 + self.n_correlated_stocks:] = True\n",
    "\n",
    "        self.prices = np.asarray(self.start_prices)\n",
    "        self.budgets = self.min_budget + self.rng.random(\n",
    "            size=(self.num_agents), dtype=\"float32\") * (\n",
    "                self.max_budget - self.min_budget)\n",
    "        self.shares = self.rng.integers(low=1,\n",
    "                                        high=self.max_shares,\n",
    "                                        size=(self.num_agents, self.n_stocks))\n",
    "\n",
    "        return (self.prices,\n",
    "                {\n",
    "                    \"correlated_stocks\": correlated_stocks,\n",
    "                    \"uncorrelated_stocks\": uncorrelated_stocks,\n",
    "                    \"budgets\": self.budgets,\n",
    "                    \"shares\": self.shares,\n",
    "                    \"valid_mask\": self.valid_mask,\n",
    "                    \"company_states\": None  # TODO: Company states\n",
    "                })\n",
    "    \n",
    "    def is_terminated(self) -> bool:\n",
    "        return True\n",
    "\n",
    "    def step(self, action: Tuple[np.ndarray, np.ndarray]) -> Tuple:\n",
    "        assert (len(action) == 2 and\n",
    "                action[0].shape == (self.num_agents, self.n_stocks) and\n",
    "                action[1].shape == (self.num_agents, self.n_stocks))\n",
    "        # TODO\n",
    "        proposed_prices = 1. + np.exp(action[0])\n",
    "        proposed_shares = action[1]\n",
    "\n",
    "        # Update budgets and shares\n",
    "        potential_budgets = self.budgets + \\\n",
    "            (proposed_prices * (-proposed_shares)).sum(-1)\n",
    "        potential_shares = self.shares + proposed_shares\n",
    "        print(\"Current budgets: \\n\", potential_budgets,\n",
    "              \"\\nCurrent shares: \\n\", potential_shares)\n",
    "        rewards = np.where(\n",
    "            np.logical_or(potential_budgets < 0.0,\n",
    "                          np.any(potential_shares < 0.0, axis=-1)),\n",
    "            -100, 0.0\n",
    "        )\n",
    "        print(\"Rewards\", rewards)\n",
    "        curr_prices = self.prices\n",
    "        \n",
    "        # TODO\n",
    "\n",
    "\n",
    "        # TODO\n",
    "\n",
    "        dones = self.is_terminated()\n",
    "        if dones:\n",
    "            next_s, _ = self.reset()\n",
    "\n",
    "        return \n",
    "\n",
    "    @staticmethod\n",
    "    def utility(c: float, eta: float) -> float:\n",
    "        if eta!= 1.0:\n",
    "            return (c ** (1.0 - eta) - 1.0) / (1.0 - eta)\n",
    "        else:\n",
    "            return np.log(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = StockMarketEnv(10)\n",
    "env.reset()\n",
    "random_action = env.action_space.sample()\n",
    "\n",
    "env.step(random_action)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MADDPG Trainer\n",
    "\n",
    "The `MADDPG Trainer` class is a generic version of the `DDPG` trainer initialized with\n",
    "- A sequence of `DDPG Agent` class objects\n",
    "- A shared observation buffer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "from copy import deepcopy\n",
    "from typing import Any, Optional, Sequence, Tuple, Union\n",
    "\n",
    "import numpy as np\n",
    "import torch as th\n",
    "from pettingzoo.mpe import simple_adversary_v2\n",
    "from src.memory import MADDPGReplayBuffer\n",
    "from torch import Tensor, nn, optim\n",
    "from torch.nn import functional as F\n",
    "\n",
    "device = th.device('cuda:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model\n",
    "class PolicyNet(nn.Module):\n",
    "\n",
    "    def __init__(self,\n",
    "                 in_features: int,\n",
    "                 action_size: int,\n",
    "                 num_hidden_1: int = 400,\n",
    "                 num_hidden_2: int = 300,\n",
    "                 negative_slope: float = 0.01) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.linear_1 = nn.Linear(in_features, num_hidden_1)\n",
    "        self.linear_2 = nn.Linear(num_hidden_1, num_hidden_2)\n",
    "        self.linear_3 = nn.Linear(num_hidden_2, action_size)\n",
    "        self.neg_slope = negative_slope\n",
    "\n",
    "        self.reset_parameters()\n",
    "    \n",
    "    def forward(self, obs: Tensor) -> Tensor:\n",
    "        obs = obs.float()\n",
    "        obs = F.leaky_relu(self.linear_1(obs), self.neg_slope)\n",
    "        obs = F.leaky_relu(self.linear_2(obs), self.neg_slope)\n",
    "        acs = th.tanh(self.linear_3(obs))\n",
    "        \n",
    "        return acs\n",
    "    \n",
    "    def reset_parameters(self) -> None:\n",
    "        gain_lrelu = nn.init.calculate_gain('leaky_relu')\n",
    "        gain_tanh = nn.init.calculate_gain('tanh')\n",
    "        nn.init.xavier_uniform_(self.linear_1.weight, gain=gain_lrelu)\n",
    "        nn.init.xavier_uniform_(self.linear_2.weight, gain=gain_lrelu)\n",
    "        nn.init.xavier_uniform_(self.linear_3.weight, gain=gain_tanh)\n",
    "\n",
    "\n",
    "class CriticNet(nn.Module):\n",
    "    \n",
    "    def __init__(self,\n",
    "                 obs_in_features: int,\n",
    "                 acs_in_features: int,\n",
    "                 num_hidden_1: int = 400,\n",
    "                 num_hidden_2: int = 300,\n",
    "                 negative_slope: float = 0.01) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.linear_1 = nn.Linear(obs_in_features, num_hidden_1)\n",
    "        self.linear_2 = nn.Linear(num_hidden_1 + acs_in_features, num_hidden_2)\n",
    "        self.linear_3 = nn.Linear(num_hidden_2, 1)\n",
    "        self.neg_slope = negative_slope\n",
    "\n",
    "    def forward(self, obs: Tensor, acs: Tensor) -> Tensor:\n",
    "        obs = obs.float()\n",
    "        acs = acs.float()\n",
    "\n",
    "        obs = F.leaky_relu(self.linear_1(obs), self.neg_slope)\n",
    "        q_val = F.leaky_relu(self.linear_2(th.cat([obs, acs], -1)),\n",
    "                             self.neg_slope)\n",
    "        q_val = self.linear_3(q_val)\n",
    "\n",
    "        return q_val\n",
    "    \n",
    "    def reset_parameters(self) -> None:\n",
    "        gain = nn.init.calculate_gain('leaky_relu')\n",
    "        nn.init.xavier_uniform_(self.linear_1.weight, gain)\n",
    "        nn.init.xavier_uniform_(self.linear_2.weight, gain)\n",
    "        nn.init.xavier_uniform_(self.linear_3.weight, gain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = simple_adversary_v2.parallel_env(max_cycles=25)\n",
    "env.reset()\n",
    "# Initialize agents\n",
    "def hard_update(src: nn.Module,\n",
    "                tar: nn.Module,\n",
    "                non_blocking: bool = True) -> None:\n",
    "    with th.no_grad():\n",
    "        for param, tar_param in zip(src.parameters(), tar.parameters()):\n",
    "            param.data.copy_(tar_param, non_blocking)\n",
    "\n",
    "def soft_update(src: nn.Module,\n",
    "                tar: nn.Module,\n",
    "                tau: float = 0.001,\n",
    "                non_blocking: bool = True) -> None:\n",
    "    with th.no_grad():\n",
    "        for param, tar_param in zip(src.parameters(), tar.parameters()):\n",
    "            param.data.copy_(param * tau + tar_param * (1 - tau))\n",
    "\n",
    "policy_nets, critic_nets = {}, {} \n",
    "policy_tar_nets, critic_tar_nets = {}, {}\n",
    "policy_opts, critic_opts = {}, {}\n",
    "global_obs_size, global_acs_size = 0, 0\n",
    "for agent in env.agents:\n",
    "    if len(env.observation_space(agent).shape) > 2:\n",
    "        raise RuntimeError('Image inputs not supported')\n",
    "    global_obs_size += env.observation_space(agent).shape[0]\n",
    "    global_acs_size += env.action_space(agent).n\n",
    "\n",
    "for agent in env.agents:\n",
    "    policy_nets[agent] = PolicyNet(env.observation_space(agent).shape[0],\n",
    "                                   env.action_space(agent).n).to(device)\n",
    "    policy_tar_nets[agent] = PolicyNet(env.observation_space(agent).shape[0],\n",
    "                                       env.action_space(agent).n).to(device)\n",
    "    hard_update(policy_tar_nets[agent], policy_nets[agent])\n",
    "    critic_nets[agent] = \\\n",
    "        CriticNet(global_obs_size, global_acs_size).to(device)\n",
    "    critic_tar_nets[agent] = \\\n",
    "        CriticNet(global_obs_size, global_acs_size).to(device)\n",
    "    hard_update(critic_tar_nets[agent], critic_nets[agent])\n",
    "    policy_opts[agent] = optim.Adam(policy_nets[agent].parameters(), lr=1e-4)\n",
    "    critic_opts[agent] = \\\n",
    "        optim.Adam(critic_nets[agent].parameters(), lr=1e-3, weight_decay=1e-2)\n",
    "\n",
    "buffer = MultiAgentReplayBuffer(env.agents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "batch_size: int = 64\n",
    "discount: float = 0.99\n",
    "max_episode_step: int = 500\n",
    "num_episodes: int = 2000\n",
    "num_warm_up: int = 400"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Loop\n",
    "# =========================================\n",
    "n_agents = len(env.agents)\n",
    "agent_rews = np.empty(shape=(num_episodes, n_agents), dtype='float32')\n",
    "episode_rews = np.empty(shape=(num_episodes, 1), dtype='float32')\n",
    "env_step: int = 0\n",
    "\n",
    "def to_one_hot(data: int, num_classes: int = -1) -> np.ndarray:\n",
    "    if num_classes == -1:\n",
    "        num_classes = int(max(data) + 1)\n",
    "\n",
    "    if isinstance(data, int):\n",
    "        output = np.zeros(shape=(num_classes,))\n",
    "        output[data] = 1\n",
    "    else:\n",
    "        output = data\n",
    "\n",
    "    return output\n",
    "\n",
    "    \n",
    "for episode in range(num_episodes):\n",
    "    ob_n = env.reset()\n",
    "    while env.agents:\n",
    "        env_step += 1\n",
    "        if env_step <= num_warm_up:\n",
    "            actions = {_a: env.action_space(_a).sample() for _a in env.agents}\n",
    "            ac_n = {_a: to_one_hot(ac, env.action_space(_a).n)\n",
    "                    for _a, ac in actions.items()}\n",
    "        else:\n",
    "            actions, ac_n = {}, {}\n",
    "            for agent, ob in ob_n.items():\n",
    "                ob = th.from_numpy(ob).view(1, -1).float().to(device)\n",
    "                ac = F.gumbel_softmax(policy_nets[agent].forward(ob))\n",
    "                actions[agent] = ac.view(-1).argmax().item()\n",
    "                ac_n[agent] = ac.detach().cpu().numpy()\n",
    "            \n",
    "        next_ob_n, rew_n, done_n, _, _ = env.step(actions)\n",
    "        buffer.add_transition(ob_n, ac_n, next_ob_n, rew_n, done_n)\n",
    "\n",
    "        # Learn\n",
    "        if len(buffer) > batch_size:\n",
    "            for _id in env.agents:\n",
    "                obs_n, acs_n, next_obs_n, rew_n, dones_n = \\\n",
    "                    buffer.sample(batch_size, random=True, device=device) \n",
    "\n",
    "                # Centralized observation\n",
    "                states = th.hstack(list(obs_n.values()))\n",
    "                next_states = th.hstack(list(next_obs_n.values()))\n",
    "                actions = th.hstack(list(acs_n.values()))\n",
    "                next_actions = th.hstack(\n",
    "                    [F.gumbel_softmax(\n",
    "                        policy_tar_nets[_id].forward(next_obs_n[_id]),\n",
    "                        hard=True\n",
    "                    ).detach()\n",
    "                            for _id in env.agents]\n",
    "                )\n",
    "\n",
    "            \n",
    "                ob, ac, next_ob, rew, done = (\n",
    "                    obs_n[_id],\n",
    "                    acs_n[_id],\n",
    "                    next_ob_n[_id],\n",
    "                    rew_n[_id],\n",
    "                    dones_n[_id]\n",
    "                )                \n",
    "\n",
    "                # Update critic network\n",
    "                rew = rew.view(-1, 1)\n",
    "                done = done.view(-1, 1).to(device)\n",
    "                actions = th.hstack(list(acs_n.values())).to(device)\n",
    "                q_val = critic_nets[_id].forward(states, actions)\n",
    "                tar_q_val = rew + discount * (1 - done) * \\\n",
    "                    critic_tar_nets[_id](next_states, next_actions)\n",
    "                critic_opts[_id].zero_grad()\n",
    "                closs = F.mse_loss(q_val, tar_q_val.detach(), reduction='mean')\n",
    "                closs.backward()\n",
    "                critic_opts[_id].step()\n",
    "\n",
    "                # Update policy network\n",
    "                ob = ob.to(device)\n",
    "                new_logits = policy_nets[_id].forward(ob)\n",
    "                new_action = F.gumbel_softmax(new_logits, hard=True)\n",
    "                acs_n[_id] = new_action\n",
    "                policy_opts[_id].zero_grad()\n",
    "                aloss = -critic_nets[_id].forward(\n",
    "                    states, th.hstack(list(acs_n.values())))\n",
    "                policy_opts[_id].step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = policy_nets['agent_0'].forward(th.from_numpy(ob).to(device))\n",
    "F.gumbel_softmax(logits, hard=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Dec 12] Update Multi-agent Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import math\n",
    "import numpy as np\n",
    "import torch as th\n",
    "\n",
    "from stock_market.ddpg import DDPGAgent\n",
    "from stock_market.env import StockMarketEnv\n",
    "from stock_market.trainer import MADDPGTrainer\n",
    "from stock_market.utils import process_sample_ac, process_step_ac"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3767.5508 -46294 3767\n",
      "855.7975 -12993 855\n",
      "4748.6016 -68648 4748\n",
      "7977.2314 -47571 7977\n",
      "1975.7654 -33020 1975\n"
     ]
    }
   ],
   "source": [
    "env = StockMarketEnv(num_agents=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'agent_0': (array([2.3122523], dtype=float32), -3628), 'agent_1': (array([1.3093233], dtype=float32), -6048), 'agent_2': (array([1.2247305], dtype=float32), -56373), 'agent_3': (array([1.3484704], dtype=float32), -38358), 'agent_4': (array([1.4173177], dtype=float32), -18073)}\n"
     ]
    }
   ],
   "source": [
    "actions = {_id: env.action_space(_id).sample() for _id in env.unwrapped.agents}\n",
    "print(actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'agent_0': array([[ 2.65247792e-01, -3.62800000e+03]]),\n",
       " 'agent_1': array([[-8.25348914e-01, -6.04800000e+03]]),\n",
       " 'agent_2': array([[-9.0384841e-01, -5.6373000e+04]]),\n",
       " 'agent_3': array([[-7.8343457e-01, -3.8358000e+04]]),\n",
       " 'agent_4': array([[-7.03353941e-01, -1.80730000e+04]])}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{_id: process_sample_ac(ac, env.unwrapped.action_space(_id))\n",
    " for _id, ac in actions.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'agent_0': (array([1.36310543, 4.15555221, 1.4406315 , 1.58251473, 1.5251726 ,\n",
       "         1.15617853, 1.92743703, 1.96731263, 1.16085011, 1.40648424,\n",
       "         2.17412474, 1.73790525, 1.69544223, 2.22366209, 3.287363  ,\n",
       "         1.97304385, 1.25184426, 1.5598287 , 1.92963193, 1.03602352,\n",
       "         1.82373738, 1.3749656 , 1.91461383, 4.48738329, 1.2474335 ,\n",
       "         1.94295237, 1.60554977, 1.54975909, 2.61459812, 1.19843495,\n",
       "         1.47009535, 1.36095952, 1.68975067, 1.35430823, 1.5296132 ,\n",
       "         2.00176432, 4.41767427, 1.33026938, 1.24567845, 1.93416856,\n",
       "         1.46254061, 1.42914557, 1.59934312, 2.85280621, 4.12297798,\n",
       "         2.70473054, 1.62320327, 2.12633414, 4.27980936, 6.34470501,\n",
       "         1.53506467, 1.56895045, 3.93381766, 3.12119372, 1.53507991,\n",
       "         1.83926002, 1.36544809, 2.4993754 , 2.03554403, 2.29631954,\n",
       "         1.97684134, 1.94850654, 1.90213507, 1.50916373]),\n",
       "  tensor([[-22512],\n",
       "          [-25066],\n",
       "          [ -7067],\n",
       "          [-16949],\n",
       "          [-41875],\n",
       "          [-40551],\n",
       "          [-18847],\n",
       "          [-51289],\n",
       "          [-64269],\n",
       "          [ -6799],\n",
       "          [-37133],\n",
       "          [-44677],\n",
       "          [-93978],\n",
       "          [-76012],\n",
       "          [-10517],\n",
       "          [-26014],\n",
       "          [-38259],\n",
       "          [-30076],\n",
       "          [-52256],\n",
       "          [-83108],\n",
       "          [-40919],\n",
       "          [-34878],\n",
       "          [-65716],\n",
       "          [-40346],\n",
       "          [-79000],\n",
       "          [-19803],\n",
       "          [-22885],\n",
       "          [-70464],\n",
       "          [-72516],\n",
       "          [   791],\n",
       "          [  1429],\n",
       "          [  3765],\n",
       "          [-36102],\n",
       "          [-71420],\n",
       "          [-40555],\n",
       "          [-45461],\n",
       "          [-17443],\n",
       "          [-66825],\n",
       "          [-40161],\n",
       "          [-45999],\n",
       "          [-87793],\n",
       "          [-14595],\n",
       "          [-90921],\n",
       "          [-75623],\n",
       "          [-63841],\n",
       "          [-50262],\n",
       "          [-18892],\n",
       "          [-26916],\n",
       "          [-81753],\n",
       "          [-77693],\n",
       "          [-92975],\n",
       "          [-24570],\n",
       "          [-12918],\n",
       "          [-79001],\n",
       "          [    87],\n",
       "          [-44994],\n",
       "          [-65418],\n",
       "          [-32869],\n",
       "          [-80152],\n",
       "          [-51524],\n",
       "          [-26824],\n",
       "          [  1933],\n",
       "          [-89773],\n",
       "          [  -777]], dtype=torch.int32)),\n",
       " 'agent_1': (array([1.62856655, 1.88332791, 1.10326089, 2.75204369, 3.02079899,\n",
       "         2.69933612, 2.03113449, 2.48978589, 2.01785248, 6.88719771,\n",
       "         2.9276483 , 1.39610518, 1.58665184, 1.49426279, 1.83782854,\n",
       "         5.17365101, 3.02802755, 1.86895508, 1.78009579, 2.34089   ,\n",
       "         1.69634314, 1.5083222 , 1.47500103, 2.03622477, 1.81107897,\n",
       "         2.80852415, 1.25081721, 1.57658338, 1.47074892, 2.51383778,\n",
       "         1.50219836, 1.90051934, 1.20089648, 1.32880055, 1.39974717,\n",
       "         1.47859319, 2.10770014, 1.66857073, 1.25446958, 2.7854343 ,\n",
       "         2.24431013, 1.72365409, 2.84957103, 1.17243652, 1.66238464,\n",
       "         1.37683222, 1.5681748 , 1.24692026, 2.09514305, 3.01552038,\n",
       "         1.64299516, 1.2500923 , 1.07014701, 1.34355692, 4.06263152,\n",
       "         1.3192531 , 1.57395598, 2.39107575, 1.44359553, 2.06286347,\n",
       "         1.55359671, 5.65505292, 1.43206041, 2.17961039]),\n",
       "  tensor([[-15447],\n",
       "          [ -2554],\n",
       "          [-25801],\n",
       "          [ -8612],\n",
       "          [ -9984],\n",
       "          [ -3262],\n",
       "          [-24438],\n",
       "          [-18309],\n",
       "          [  -382],\n",
       "          [-16117],\n",
       "          [-24945],\n",
       "          [-24947],\n",
       "          [  -982],\n",
       "          [-23004],\n",
       "          [  -328],\n",
       "          [-13083],\n",
       "          [   467],\n",
       "          [-23590],\n",
       "          [-14100],\n",
       "          [-13281],\n",
       "          [  -210],\n",
       "          [ -9518],\n",
       "          [-25745],\n",
       "          [-16177],\n",
       "          [-19749],\n",
       "          [ -2052],\n",
       "          [   718],\n",
       "          [   441],\n",
       "          [ -6340],\n",
       "          [   163],\n",
       "          [   442],\n",
       "          [-25555],\n",
       "          [-26226],\n",
       "          [ -2128],\n",
       "          [ -7567],\n",
       "          [-18954],\n",
       "          [-25149],\n",
       "          [-15023],\n",
       "          [-11560],\n",
       "          [-24573],\n",
       "          [-16406],\n",
       "          [ -3486],\n",
       "          [-21392],\n",
       "          [-22480],\n",
       "          [-24895],\n",
       "          [-10298],\n",
       "          [  -525],\n",
       "          [-14063],\n",
       "          [-15654],\n",
       "          [  -632],\n",
       "          [ -3937],\n",
       "          [-18591],\n",
       "          [ -8269],\n",
       "          [ -1759],\n",
       "          [ -5134],\n",
       "          [-13545],\n",
       "          [-10641],\n",
       "          [-11722],\n",
       "          [-20898],\n",
       "          [ -2653],\n",
       "          [-17495],\n",
       "          [ -4886],\n",
       "          [-22967],\n",
       "          [-15814]], dtype=torch.int32)),\n",
       " 'agent_2': (array([1.57285725, 8.35770884, 1.59772629, 2.05975429, 2.95769374,\n",
       "         1.67503921, 3.90788301, 3.30834828, 5.59798874, 3.54958978,\n",
       "         1.04300187, 2.13016484, 1.72693758, 2.20235017, 3.31933657,\n",
       "         1.58768204, 5.95050136, 3.4452597 , 2.0885823 , 1.3506404 ,\n",
       "         1.35173638, 2.0850374 , 4.46238407, 1.81942741, 1.20879279,\n",
       "         2.30591875, 3.10966021, 1.63806736, 2.91144171, 1.33518715,\n",
       "         2.37020571, 2.26908886, 1.77738075, 2.2074206 , 7.75816503,\n",
       "         1.44702658, 2.42788301, 1.61745351, 2.98734072, 1.67736301,\n",
       "         1.58775534, 7.05525782, 1.46084437, 3.14361543, 1.63471758,\n",
       "         4.12325318, 3.04001566, 2.063299  , 1.75360563, 1.69395082,\n",
       "         4.85603365, 1.31461202, 3.15478269, 2.20510055, 1.31725949,\n",
       "         1.78482156, 1.42603678, 1.8556899 , 2.54715377, 1.87139711,\n",
       "         1.32529137, 1.88822653, 2.22304637, 1.92055391]),\n",
       "  tensor([[ -39346],\n",
       "          [ -77130],\n",
       "          [-140655],\n",
       "          [-127885],\n",
       "          [-126842],\n",
       "          [ -26650],\n",
       "          [ -38199],\n",
       "          [ -32514],\n",
       "          [-137436],\n",
       "          [-141602],\n",
       "          [ -19525],\n",
       "          [-133325],\n",
       "          [-135527],\n",
       "          [   4146],\n",
       "          [ -11327],\n",
       "          [-127008],\n",
       "          [    765],\n",
       "          [ -66029],\n",
       "          [-122352],\n",
       "          [ -52290],\n",
       "          [ -45679],\n",
       "          [-135748],\n",
       "          [ -16424],\n",
       "          [ -99086],\n",
       "          [ -63872],\n",
       "          [ -36997],\n",
       "          [   1818],\n",
       "          [ -45646],\n",
       "          [ -20721],\n",
       "          [ -96757],\n",
       "          [ -84219],\n",
       "          [ -51948],\n",
       "          [ -41507],\n",
       "          [-123648],\n",
       "          [-119261],\n",
       "          [ -47079],\n",
       "          [ -94219],\n",
       "          [ -43906],\n",
       "          [-135930],\n",
       "          [ -11915],\n",
       "          [  -8022],\n",
       "          [ -72739],\n",
       "          [ -86855],\n",
       "          [  -6637],\n",
       "          [ -97156],\n",
       "          [-118580],\n",
       "          [-109695],\n",
       "          [  -5252],\n",
       "          [ -87587],\n",
       "          [-118855],\n",
       "          [-118906],\n",
       "          [  -7234],\n",
       "          [-101651],\n",
       "          [-125542],\n",
       "          [ -60983],\n",
       "          [ -97429],\n",
       "          [ -42895],\n",
       "          [ -14593],\n",
       "          [ -20519],\n",
       "          [-134693],\n",
       "          [ -65601],\n",
       "          [ -12606],\n",
       "          [ -70287],\n",
       "          [ -19873]], dtype=torch.int32)),\n",
       " 'agent_3': (array([3.30779334, 1.07728348, 2.17183545, 2.19395007, 1.33199793,\n",
       "         1.25825179, 1.94132078, 1.61075081, 1.99335191, 1.77786211,\n",
       "         1.57101543, 1.40978943, 3.15638603, 1.81569492, 2.13871299,\n",
       "         2.44354534, 2.35439815, 1.58395002, 1.34983446, 1.20718674,\n",
       "         2.69846293, 3.80153985, 8.46294734, 1.47290877, 1.50359078,\n",
       "         4.10798644, 1.48877188, 1.29911736, 3.65153709, 4.10619525,\n",
       "         2.42418188, 2.92128317, 2.95896616, 7.06897311, 1.75120926,\n",
       "         2.98121796, 1.92681912, 1.84983104, 1.46715488, 2.16487042,\n",
       "         1.27658847, 1.94225578, 1.43571188, 7.06787804, 2.08952854,\n",
       "         2.23637047, 1.17882316, 2.4979285 , 2.56032169, 1.63294934,\n",
       "         4.87554024, 1.87441322, 1.86741657, 3.10559785, 1.43269824,\n",
       "         1.444045  , 2.32426609, 2.32142536, 1.93363764, 1.90872281,\n",
       "         3.46703754, 3.10810911, 1.47824637, 2.10901127]),\n",
       "  tensor([[ -86125],\n",
       "          [ -12387],\n",
       "          [ -47987],\n",
       "          [ -98765],\n",
       "          [ -95520],\n",
       "          [ -61718],\n",
       "          [ -14189],\n",
       "          [    554],\n",
       "          [ -22656],\n",
       "          [  -7466],\n",
       "          [ -63981],\n",
       "          [ -60313],\n",
       "          [ -73787],\n",
       "          [ -67741],\n",
       "          [ -34132],\n",
       "          [  -6275],\n",
       "          [ -63510],\n",
       "          [ -48411],\n",
       "          [ -27091],\n",
       "          [ -66264],\n",
       "          [ -31714],\n",
       "          [  -3117],\n",
       "          [ -52222],\n",
       "          [ -16323],\n",
       "          [ -61566],\n",
       "          [-103117],\n",
       "          [ -14635],\n",
       "          [ -86131],\n",
       "          [  -1485],\n",
       "          [  -4035],\n",
       "          [ -99740],\n",
       "          [ -93887],\n",
       "          [  -8459],\n",
       "          [  -1754],\n",
       "          [ -99358],\n",
       "          [ -98351],\n",
       "          [ -27202],\n",
       "          [ -73986],\n",
       "          [ -86121],\n",
       "          [ -62993],\n",
       "          [ -72037],\n",
       "          [ -60249],\n",
       "          [ -27192],\n",
       "          [ -89704],\n",
       "          [-101117],\n",
       "          [ -34381],\n",
       "          [ -83625],\n",
       "          [   2894],\n",
       "          [   6260],\n",
       "          [ -38602],\n",
       "          [ -63951],\n",
       "          [ -27161],\n",
       "          [ -96779],\n",
       "          [ -41942],\n",
       "          [ -79345],\n",
       "          [ -28044],\n",
       "          [   5177],\n",
       "          [ -98604],\n",
       "          [ -47634],\n",
       "          [ -74414],\n",
       "          [ -84081],\n",
       "          [ -82651],\n",
       "          [ -30099],\n",
       "          [   5968]], dtype=torch.int32)),\n",
       " 'agent_4': (array([2.02580786, 2.29557883, 1.31132521, 2.0022212 , 1.21875295,\n",
       "         1.41622298, 1.59088823, 2.18115449, 2.52667423, 1.1597511 ,\n",
       "         2.13489165, 1.93075691, 2.18787235, 1.80301392, 2.43079651,\n",
       "         3.31309464, 2.35695757, 2.9163567 , 1.54719607, 1.49409582,\n",
       "         3.76607821, 1.29700414, 1.40907225, 1.8284105 , 3.41167699,\n",
       "         1.90728896, 4.20588567, 1.94613742, 2.27377228, 2.08621741,\n",
       "         3.29222231, 3.48730409, 3.95346626, 1.03438036, 2.15838566,\n",
       "         2.42278041, 2.15605214, 5.33383434, 1.0433559 , 3.25033315,\n",
       "         1.15200164, 3.03471747, 3.15945059, 1.63016053, 1.35905053,\n",
       "         4.72848055, 1.98873697, 1.62230115, 2.10354318, 2.87522362,\n",
       "         1.27958061, 1.66903941, 5.27554367, 3.27583064, 1.95463108,\n",
       "         1.96286675, 1.34775699, 3.28601758, 2.90777744, 1.74433953,\n",
       "         1.99649928, 1.65172619, 2.40773747, 1.54749363]),\n",
       "  tensor([[-30694],\n",
       "          [-48530],\n",
       "          [-33382],\n",
       "          [-32595],\n",
       "          [  1501],\n",
       "          [ -9554],\n",
       "          [-12849],\n",
       "          [-54452],\n",
       "          [-44422],\n",
       "          [-62760],\n",
       "          [ -2773],\n",
       "          [-61863],\n",
       "          [-53351],\n",
       "          [-30871],\n",
       "          [-63978],\n",
       "          [-17299],\n",
       "          [ -1204],\n",
       "          [-36880],\n",
       "          [  1647],\n",
       "          [-33296],\n",
       "          [-22172],\n",
       "          [-30675],\n",
       "          [-24986],\n",
       "          [-44616],\n",
       "          [-26849],\n",
       "          [ -6466],\n",
       "          [-32429],\n",
       "          [-67035],\n",
       "          [-29141],\n",
       "          [-26650],\n",
       "          [-12469],\n",
       "          [-46827],\n",
       "          [-59904],\n",
       "          [-38651],\n",
       "          [ -3776],\n",
       "          [-63577],\n",
       "          [-66478],\n",
       "          [-31898],\n",
       "          [-10394],\n",
       "          [-13040],\n",
       "          [-60013],\n",
       "          [-40156],\n",
       "          [-48269],\n",
       "          [-24727],\n",
       "          [-33576],\n",
       "          [-66587],\n",
       "          [ -8416],\n",
       "          [-50579],\n",
       "          [-44140],\n",
       "          [-59561],\n",
       "          [-27360],\n",
       "          [-26710],\n",
       "          [-65708],\n",
       "          [-64836],\n",
       "          [-16735],\n",
       "          [-50678],\n",
       "          [-27535],\n",
       "          [-52730],\n",
       "          [-12857],\n",
       "          [-25933],\n",
       "          [-44799],\n",
       "          [-26196],\n",
       "          [-59560],\n",
       "          [-39941]], dtype=torch.int32))}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acs = {_id: th.from_numpy(np.random.uniform(-1, 1, size=(64, 2)))\n",
    "       for _id in actions.keys()}\n",
    "{_id: process_step_ac(ac, env.unwrapped.action_space(_id))\n",
    " for _id, ac in acs.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'agent_0': (array([1.8858149], dtype=float32), -3961),\n",
       " 'agent_1': (array([1.0301331], dtype=float32), -8983),\n",
       " 'agent_2': (array([3.8545728], dtype=float32), -53604),\n",
       " 'agent_3': (array([2.2117112], dtype=float32), -43554),\n",
       " 'agent_4': (array([1.6507716], dtype=float32), 1006)}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3767.55078125 -46294 3767\n",
      "855.7974853515625 -12993 855\n",
      "4748.6015625 -68648 4748\n",
      "7977.2314453125 -47571 7977\n",
      "1975.765380859375 -33020 1975\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vision/juanwu/fall_22/stock_market/stock_market/env.py:68: RuntimeWarning: invalid value encountered in arctanh\n",
      "  return np.exp(np.arctanh(input)) + 1.0\n",
      "/home/vision/juanwu/fall_22/stock_market/stock_market/env.py:65: RuntimeWarning: invalid value encountered in arctanh\n",
      "  np.exp(np.arctanh(input)) + 1.0, budget / bid_share\n"
     ]
    }
   ],
   "source": [
    "next_ob_n, rew_n, done_n, _, _ = env.step(actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.00000000e+02, 1.15853969e+02, 0.00000000e+00, 0.00000000e+00,\n",
       "       1.45677524e+02, 0.00000000e+00, 6.36946153e+01, 6.18262106e+01,\n",
       "       0.00000000e+00, 0.00000000e+00, 5.30597660e+01, 1.01249412e+02,\n",
       "       1.48074666e+02, 1.44653118e+02, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 1.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 1.39977423e+02, 9.45209446e+00,\n",
       "       6.21837446e+01, 0.00000000e+00, 0.00000000e+00, 1.73751557e+02,\n",
       "       6.63823200e+00, 7.94562442e+01, 3.12897046e+03, 4.63210000e+04])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ob_n['agent_0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Discrete(200001, start=-100000)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.unwrapped._action_spaces['agent_0'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3128.9705 -46321 3128\n",
      "8105.741 -85661 8105\n",
      "5834.2725 -1953 5834\n",
      "5514.3813 -75852 5514\n",
      "1850.0498 -49276 1850\n"
     ]
    }
   ],
   "source": [
    "for i, agent in enumerate(env.unwrapped.agents):\n",
    "    # Update shares available for the agents\n",
    "    budget = env.unwrapped.budgets[i]\n",
    "    min_share = -env.unwrapped.shares[i]  # can at most sell all they have\n",
    "    max_share = math.floor(budget / (1.0 + 1e-8)) # can at most buy at $1\n",
    "    print(budget, min_share, max_share)\n",
    "\n",
    "    # Update price available for the agents\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.13 ('juanwu_cs285')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "dabfb9edd6f0b01bcba20c8b5c22bed4c56635c47168437081608022d6be55db"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
