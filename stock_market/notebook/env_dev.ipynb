{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-Agent Deep Deterministic Policy Gradient for Stock Market"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stock Market Environment\n",
    "\n",
    "- __Hyperparameters__\n",
    "- __Observation Space__\n",
    "  - `stock_price`: `ndarray` of shape $[N_{stock}, ]$\n",
    "  - `correlated_stock`: `ndarray` of shape $[N_{correlated}, ]$\n",
    "  - `uncorrelated_stock`: `ndarray` of shape $[N_{uncorrelated}, ]$\n",
    "  - `budgets`: `ndarray` of shape $[N_{agents}, ]$\n",
    "  - `shares_held`: `ndarray` of shape $[N_{agents}, ]$\n",
    "  - `agent_views`: `ndarray` of shape $[N_{agents}, N_{stock}]$\n",
    "  - `company_states`: `ndarray` of shape $[N_{company}, ]$\n",
    "- __Action Space__\n",
    "  - dimension_1: log buy/sell prices $\\log p\\in\\left(-\\infty, +\\infty\\right)$ => `gym.spaces.Box`\n",
    "  - dimension_2: discrete shares $s\\in\\mathbb{N}$ => `gym.spaces.Discrete`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, Optional, Sequence, Tuple, Union\n",
    "\n",
    "import numpy as np\n",
    "from gym.core import ActType, ObsType, Env\n",
    "from gym.spaces import Box, MultiDiscrete, Tuple as TupleSpace\n",
    "\n",
    "\n",
    "class StockMarketEnv(Env):\n",
    "    \n",
    "    def __init__(self,\n",
    "                 num_agents: int,\n",
    "                 budget_discount: float = 0.9,\n",
    "                 num_company: int = 5,\n",
    "                 num_correlated_stocks: int = 19,\n",
    "                 num_uncorrelated_stocks: int = 10,\n",
    "                 max_shares: int = 100000,\n",
    "                 start_prices: Union[float, Sequence[float]] = 100.0,\n",
    "                 min_budget: float = 100.0,\n",
    "                 max_budget: float = 10000.0,\n",
    "                 step_size: float = 1.0,\n",
    "                 price_std: float = 100.0,\n",
    "                 noise_std: float = 10.0,\n",
    "                 seed: int = 0) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        # Agent Parameters\n",
    "        self.num_agents = num_agents\n",
    "        self.num_company = num_company\n",
    "        self.min_budget = min_budget\n",
    "        self.max_budget = max_budget\n",
    "        self.budget_discount = budget_discount\n",
    "        self.max_shares = max_shares\n",
    "\n",
    "        # Stock Market Parameters\n",
    "        self.dt = step_size\n",
    "        self.start_prices = start_prices\n",
    "        self.price_std = price_std\n",
    "        self.noise_std = noise_std\n",
    "\n",
    "        # Observation and Action spaces\n",
    "        self.n_correlated_stocks = num_correlated_stocks\n",
    "        self.n_uncorrelated_stocks = num_uncorrelated_stocks\n",
    "        self.n_stocks = num_correlated_stocks + num_uncorrelated_stocks + 1\n",
    "        self.observation_space = Box(low=0.0,\n",
    "                                     high=float(\"inf\"),\n",
    "                                     shape=(self.num_agents, self.n_stocks))\n",
    "        self.action_space = TupleSpace(\n",
    "            (Box(low=-float(\"inf\"),\n",
    "                 high=float(\"inf\"),\n",
    "                 shape=(self.num_agents, self.n_stocks)),\n",
    "             MultiDiscrete([[max_shares] * self.n_stocks] * self.num_agents))\n",
    "        )\n",
    "        self._seed = seed\n",
    "\n",
    "    def reset(self,\n",
    "              seed: Optional[int] = None,\n",
    "              return_info: bool = True) -> Tuple[ObsType, Dict]:\n",
    "        self.rng = np.random.default_rng(seed=seed or self._seed)        \n",
    "\n",
    "        correlated_stocks = np.clip(\n",
    "            np.random.normal(loc=self.start_prices,\n",
    "                             scale=self.price_std,\n",
    "                             size=(self.n_correlated_stocks, )),\n",
    "            a_min=1, a_max=None\n",
    "        )\n",
    "        uncorrelated_stocks = np.clip(\n",
    "            np.random.normal(loc=self.start_prices,\n",
    "                             scale=self.price_std,\n",
    "                             size=(self.n_uncorrelated_stocks,)),\n",
    "            a_min=1, a_max=None\n",
    "        )\n",
    "        self.eta = np.clip(\n",
    "            np.random.normal(loc=1.5, scale=1.5, size=(self.num_agents, )),\n",
    "            a_min=0, a_max=10\n",
    "        )\n",
    "        self.valid_mask = np.zeros(shape=(self.num_agents, self.n_stocks),\n",
    "                                   dtype=\"bool\")\n",
    "        self.valid_mask[:, 1:1+self.n_correlated_stocks] = True\n",
    "        self.valid_mask[self.rng.integers(low=0, high=self.num_agents),\n",
    "                        1 + self.n_correlated_stocks:] = True\n",
    "\n",
    "        self.prices = np.asarray(self.start_prices)\n",
    "        self.budgets = self.min_budget + self.rng.random(\n",
    "            size=(self.num_agents), dtype=\"float32\") * (\n",
    "                self.max_budget - self.min_budget)\n",
    "        self.shares = self.rng.integers(low=1,\n",
    "                                        high=self.max_shares,\n",
    "                                        size=(self.num_agents, self.n_stocks))\n",
    "\n",
    "        return (self.prices,\n",
    "                {\n",
    "                    \"correlated_stocks\": correlated_stocks,\n",
    "                    \"uncorrelated_stocks\": uncorrelated_stocks,\n",
    "                    \"budgets\": self.budgets,\n",
    "                    \"shares\": self.shares,\n",
    "                    \"valid_mask\": self.valid_mask,\n",
    "                    \"company_states\": None  # TODO: Company states\n",
    "                })\n",
    "    \n",
    "    def is_terminated(self) -> bool:\n",
    "        return True\n",
    "\n",
    "    def step(self, action: Tuple[np.ndarray, np.ndarray]) -> Tuple:\n",
    "        assert (len(action) == 2 and\n",
    "                action[0].shape == (self.num_agents, self.n_stocks) and\n",
    "                action[1].shape == (self.num_agents, self.n_stocks))\n",
    "        # TODO\n",
    "        proposed_prices = 1. + np.exp(action[0])\n",
    "        proposed_shares = action[1]\n",
    "\n",
    "        # Update budgets and shares\n",
    "        potential_budgets = self.budgets + \\\n",
    "            (proposed_prices * (-proposed_shares)).sum(-1)\n",
    "        potential_shares = self.shares + proposed_shares\n",
    "        print(\"Current budgets: \\n\", potential_budgets,\n",
    "              \"\\nCurrent shares: \\n\", potential_shares)\n",
    "        rewards = np.where(\n",
    "            np.logical_or(potential_budgets < 0.0,\n",
    "                          np.any(potential_shares < 0.0, axis=-1)),\n",
    "            -100, 0.0\n",
    "        )\n",
    "        print(\"Rewards\", rewards)\n",
    "        curr_prices = self.prices\n",
    "        \n",
    "        # TODO\n",
    "\n",
    "\n",
    "        # TODO\n",
    "\n",
    "        dones = self.is_terminated()\n",
    "        if dones:\n",
    "            next_s, _ = self.reset()\n",
    "\n",
    "        return \n",
    "\n",
    "    @staticmethod\n",
    "    def utility(c: float, eta: float) -> float:\n",
    "        if eta!= 1.0:\n",
    "            return (c ** (1.0 - eta) - 1.0) / (1.0 - eta)\n",
    "        else:\n",
    "            return np.log(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = StockMarketEnv(10)\n",
    "env.reset()\n",
    "random_action = env.action_space.sample()\n",
    "\n",
    "env.step(random_action)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MADDPG Trainer\n",
    "\n",
    "The `MADDPG Trainer` class is a generic version of the `DDPG` trainer initialized with\n",
    "- A sequence of `DDPG Agent` class objects\n",
    "- A shared observation buffer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "from copy import deepcopy\n",
    "from typing import Any, Optional, Sequence, Tuple, Union\n",
    "\n",
    "import numpy as np\n",
    "import torch as th\n",
    "from pettingzoo.mpe import simple_adversary_v2\n",
    "from src.memory import MADDPGReplayBuffer\n",
    "from torch import Tensor, nn, optim\n",
    "from torch.nn import functional as F\n",
    "\n",
    "device = th.device('cuda:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model\n",
    "class PolicyNet(nn.Module):\n",
    "\n",
    "    def __init__(self,\n",
    "                 in_features: int,\n",
    "                 action_size: int,\n",
    "                 num_hidden_1: int = 400,\n",
    "                 num_hidden_2: int = 300,\n",
    "                 negative_slope: float = 0.01) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.linear_1 = nn.Linear(in_features, num_hidden_1)\n",
    "        self.linear_2 = nn.Linear(num_hidden_1, num_hidden_2)\n",
    "        self.linear_3 = nn.Linear(num_hidden_2, action_size)\n",
    "        self.neg_slope = negative_slope\n",
    "\n",
    "        self.reset_parameters()\n",
    "    \n",
    "    def forward(self, obs: Tensor) -> Tensor:\n",
    "        obs = obs.float()\n",
    "        obs = F.leaky_relu(self.linear_1(obs), self.neg_slope)\n",
    "        obs = F.leaky_relu(self.linear_2(obs), self.neg_slope)\n",
    "        acs = th.tanh(self.linear_3(obs))\n",
    "        \n",
    "        return acs\n",
    "    \n",
    "    def reset_parameters(self) -> None:\n",
    "        gain_lrelu = nn.init.calculate_gain('leaky_relu')\n",
    "        gain_tanh = nn.init.calculate_gain('tanh')\n",
    "        nn.init.xavier_uniform_(self.linear_1.weight, gain=gain_lrelu)\n",
    "        nn.init.xavier_uniform_(self.linear_2.weight, gain=gain_lrelu)\n",
    "        nn.init.xavier_uniform_(self.linear_3.weight, gain=gain_tanh)\n",
    "\n",
    "\n",
    "class CriticNet(nn.Module):\n",
    "    \n",
    "    def __init__(self,\n",
    "                 obs_in_features: int,\n",
    "                 acs_in_features: int,\n",
    "                 num_hidden_1: int = 400,\n",
    "                 num_hidden_2: int = 300,\n",
    "                 negative_slope: float = 0.01) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.linear_1 = nn.Linear(obs_in_features, num_hidden_1)\n",
    "        self.linear_2 = nn.Linear(num_hidden_1 + acs_in_features, num_hidden_2)\n",
    "        self.linear_3 = nn.Linear(num_hidden_2, 1)\n",
    "        self.neg_slope = negative_slope\n",
    "\n",
    "    def forward(self, obs: Tensor, acs: Tensor) -> Tensor:\n",
    "        obs = obs.float()\n",
    "        acs = acs.float()\n",
    "\n",
    "        obs = F.leaky_relu(self.linear_1(obs), self.neg_slope)\n",
    "        q_val = F.leaky_relu(self.linear_2(th.cat([obs, acs], -1)),\n",
    "                             self.neg_slope)\n",
    "        q_val = self.linear_3(q_val)\n",
    "\n",
    "        return q_val\n",
    "    \n",
    "    def reset_parameters(self) -> None:\n",
    "        gain = nn.init.calculate_gain('leaky_relu')\n",
    "        nn.init.xavier_uniform_(self.linear_1.weight, gain)\n",
    "        nn.init.xavier_uniform_(self.linear_2.weight, gain)\n",
    "        nn.init.xavier_uniform_(self.linear_3.weight, gain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = simple_adversary_v2.parallel_env(max_cycles=25)\n",
    "env.reset()\n",
    "# Initialize agents\n",
    "def hard_update(src: nn.Module,\n",
    "                tar: nn.Module,\n",
    "                non_blocking: bool = True) -> None:\n",
    "    with th.no_grad():\n",
    "        for param, tar_param in zip(src.parameters(), tar.parameters()):\n",
    "            param.data.copy_(tar_param, non_blocking)\n",
    "\n",
    "def soft_update(src: nn.Module,\n",
    "                tar: nn.Module,\n",
    "                tau: float = 0.001,\n",
    "                non_blocking: bool = True) -> None:\n",
    "    with th.no_grad():\n",
    "        for param, tar_param in zip(src.parameters(), tar.parameters()):\n",
    "            param.data.copy_(param * tau + tar_param * (1 - tau))\n",
    "\n",
    "policy_nets, critic_nets = {}, {} \n",
    "policy_tar_nets, critic_tar_nets = {}, {}\n",
    "policy_opts, critic_opts = {}, {}\n",
    "global_obs_size, global_acs_size = 0, 0\n",
    "for agent in env.agents:\n",
    "    if len(env.observation_space(agent).shape) > 2:\n",
    "        raise RuntimeError('Image inputs not supported')\n",
    "    global_obs_size += env.observation_space(agent).shape[0]\n",
    "    global_acs_size += env.action_space(agent).n\n",
    "\n",
    "for agent in env.agents:\n",
    "    policy_nets[agent] = PolicyNet(env.observation_space(agent).shape[0],\n",
    "                                   env.action_space(agent).n).to(device)\n",
    "    policy_tar_nets[agent] = PolicyNet(env.observation_space(agent).shape[0],\n",
    "                                       env.action_space(agent).n).to(device)\n",
    "    hard_update(policy_tar_nets[agent], policy_nets[agent])\n",
    "    critic_nets[agent] = \\\n",
    "        CriticNet(global_obs_size, global_acs_size).to(device)\n",
    "    critic_tar_nets[agent] = \\\n",
    "        CriticNet(global_obs_size, global_acs_size).to(device)\n",
    "    hard_update(critic_tar_nets[agent], critic_nets[agent])\n",
    "    policy_opts[agent] = optim.Adam(policy_nets[agent].parameters(), lr=1e-4)\n",
    "    critic_opts[agent] = \\\n",
    "        optim.Adam(critic_nets[agent].parameters(), lr=1e-3, weight_decay=1e-2)\n",
    "\n",
    "buffer = MultiAgentReplayBuffer(env.agents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "batch_size: int = 64\n",
    "discount: float = 0.99\n",
    "max_episode_step: int = 500\n",
    "num_episodes: int = 2000\n",
    "num_warm_up: int = 400"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Loop\n",
    "# =========================================\n",
    "n_agents = len(env.agents)\n",
    "agent_rews = np.empty(shape=(num_episodes, n_agents), dtype='float32')\n",
    "episode_rews = np.empty(shape=(num_episodes, 1), dtype='float32')\n",
    "env_step: int = 0\n",
    "\n",
    "def to_one_hot(data: int, num_classes: int = -1) -> np.ndarray:\n",
    "    if num_classes == -1:\n",
    "        num_classes = int(max(data) + 1)\n",
    "\n",
    "    if isinstance(data, int):\n",
    "        output = np.zeros(shape=(num_classes,))\n",
    "        output[data] = 1\n",
    "    else:\n",
    "        output = data\n",
    "\n",
    "    return output\n",
    "\n",
    "    \n",
    "for episode in range(num_episodes):\n",
    "    ob_n = env.reset()\n",
    "    while env.agents:\n",
    "        env_step += 1\n",
    "        if env_step <= num_warm_up:\n",
    "            actions = {_a: env.action_space(_a).sample() for _a in env.agents}\n",
    "            ac_n = {_a: to_one_hot(ac, env.action_space(_a).n)\n",
    "                    for _a, ac in actions.items()}\n",
    "        else:\n",
    "            actions, ac_n = {}, {}\n",
    "            for agent, ob in ob_n.items():\n",
    "                ob = th.from_numpy(ob).view(1, -1).float().to(device)\n",
    "                ac = F.gumbel_softmax(policy_nets[agent].forward(ob))\n",
    "                actions[agent] = ac.view(-1).argmax().item()\n",
    "                ac_n[agent] = ac.detach().cpu().numpy()\n",
    "            \n",
    "        next_ob_n, rew_n, done_n, _, _ = env.step(actions)\n",
    "        buffer.add_transition(ob_n, ac_n, next_ob_n, rew_n, done_n)\n",
    "\n",
    "        # Learn\n",
    "        if len(buffer) > batch_size:\n",
    "            for _id in env.agents:\n",
    "                obs_n, acs_n, next_obs_n, rew_n, dones_n = \\\n",
    "                    buffer.sample(batch_size, random=True, device=device) \n",
    "\n",
    "                # Centralized observation\n",
    "                states = th.hstack(list(obs_n.values()))\n",
    "                next_states = th.hstack(list(next_obs_n.values()))\n",
    "                actions = th.hstack(list(acs_n.values()))\n",
    "                next_actions = th.hstack(\n",
    "                    [F.gumbel_softmax(\n",
    "                        policy_tar_nets[_id].forward(next_obs_n[_id]),\n",
    "                        hard=True\n",
    "                    ).detach()\n",
    "                            for _id in env.agents]\n",
    "                )\n",
    "\n",
    "            \n",
    "                ob, ac, next_ob, rew, done = (\n",
    "                    obs_n[_id],\n",
    "                    acs_n[_id],\n",
    "                    next_ob_n[_id],\n",
    "                    rew_n[_id],\n",
    "                    dones_n[_id]\n",
    "                )                \n",
    "\n",
    "                # Update critic network\n",
    "                rew = rew.view(-1, 1)\n",
    "                done = done.view(-1, 1).to(device)\n",
    "                actions = th.hstack(list(acs_n.values())).to(device)\n",
    "                q_val = critic_nets[_id].forward(states, actions)\n",
    "                tar_q_val = rew + discount * (1 - done) * \\\n",
    "                    critic_tar_nets[_id](next_states, next_actions)\n",
    "                critic_opts[_id].zero_grad()\n",
    "                closs = F.mse_loss(q_val, tar_q_val.detach(), reduction='mean')\n",
    "                closs.backward()\n",
    "                critic_opts[_id].step()\n",
    "\n",
    "                # Update policy network\n",
    "                ob = ob.to(device)\n",
    "                new_logits = policy_nets[_id].forward(ob)\n",
    "                new_action = F.gumbel_softmax(new_logits, hard=True)\n",
    "                acs_n[_id] = new_action\n",
    "                policy_opts[_id].zero_grad()\n",
    "                aloss = -critic_nets[_id].forward(\n",
    "                    states, th.hstack(list(acs_n.values())))\n",
    "                policy_opts[_id].step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = policy_nets['agent_0'].forward(th.from_numpy(ob).to(device))\n",
    "F.gumbel_softmax(logits, hard=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Dec 12] Update Multi-agent Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from stock_market.ddpg import DDPGAgent\n",
    "from stock_market.env import LogarithmAndIntActionWrapper, StockMarketEnv\n",
    "from stock_market.trainer import MADDPGTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = LogarithmAndIntActionWrapper(StockMarketEnv(num_agents=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.13 ('juanwu_cs285')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "dabfb9edd6f0b01bcba20c8b5c22bed4c56635c47168437081608022d6be55db"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
